{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "  import google.colab\n",
    "  IN_COLAB = True\n",
    "except:\n",
    "  IN_COLAB = False\n",
    "\n",
    "# keras module for building LSTM \n",
    "from keras.layers import Embedding, LSTM, Dense, Dropout, Flatten, Bidirectional\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.callbacks import EarlyStopping\n",
    "from keras.models import Sequential\n",
    "import keras.utils as ku \n",
    "import keras\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.utils import Sequence\n",
    "\n",
    "if IN_COLAB:\n",
    "  !pip install Keras-Preprocessing\n",
    "from keras_preprocessing.sequence import pad_sequences\n",
    "from keras.preprocessing.text import text_to_word_sequence\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import string, os "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Text Generation with LSTM\n",
    "\n",
    "As we have seen, LSTM models are excellent at dealing with sequential data. As luck would have it, text is also sequental data! We can train a model to predict the next word in a sentence, then use that smarts to generate new text. Basically ChatGPT, but far better. \n",
    "\n",
    "### Text for Training\n",
    "\n",
    "We need some text from which to train our model to speak, I captured a small extract of text from Reddit posts, which vaguely resembles actual language. We'll first need to clean up our data a bit before we can assemble it for modelling. The inital cleaning bits are just like what we used in NLP, we just need to get rid of all the junk. \n",
    "\n",
    "We can use pretty much anything that you can imagine as source, and assuming we can gather enough data and train our model, the generated speech will be styled after the source. I liken it to going on vacation in Indonesia and talking to Indonesians who spoke English like Australian surfer bros - their training data was a little weird, so the output was a little weird too. If you're looking to build your best ChatGPT competitor you will want a lot of data, specifically a lot of data that is representative of the full gamut of how you want your model to write. If you want slang in the new text, you can't really train on Shakespeare and Wikipedia."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>title</th>\n",
       "      <th>score</th>\n",
       "      <th>id</th>\n",
       "      <th>url</th>\n",
       "      <th>comms_num</th>\n",
       "      <th>created</th>\n",
       "      <th>body</th>\n",
       "      <th>timestamp</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>36207</th>\n",
       "      <td>I am seated on the AMC rocket awaiting take of...</td>\n",
       "      <td>137</td>\n",
       "      <td>lsqlri</td>\n",
       "      <td>https://i.redd.it/oltiuvk6arj61.jpg</td>\n",
       "      <td>42</td>\n",
       "      <td>1.614345e+09</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2021-02-26 15:14:42</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25558</th>\n",
       "      <td>Unpinned Daily Discussion Thread for February ...</td>\n",
       "      <td>0</td>\n",
       "      <td>lcdbiy</td>\n",
       "      <td>https://www.reddit.com/r/wallstreetbets/commen...</td>\n",
       "      <td>1501</td>\n",
       "      <td>1.612465e+09</td>\n",
       "      <td>Your daily trading discussion thread. Please k...</td>\n",
       "      <td>2021-02-04 21:00:22</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4865</th>\n",
       "      <td>Go downvote Trading 212 on Play Store, Can't g...</td>\n",
       "      <td>2</td>\n",
       "      <td>l6zifm</td>\n",
       "      <td>https://www.reddit.com/r/wallstreetbets/commen...</td>\n",
       "      <td>0</td>\n",
       "      <td>1.611876e+09</td>\n",
       "      <td>Fuck them up</td>\n",
       "      <td>2021-01-29 01:22:16</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11012</th>\n",
       "      <td>THE SQUEEZE HASN'T STARTED YET THIS IS JUST A ...</td>\n",
       "      <td>1</td>\n",
       "      <td>l71hrs</td>\n",
       "      <td>https://i.redd.it/hnzd7uy9o3e61.jpg</td>\n",
       "      <td>8</td>\n",
       "      <td>1.611880e+09</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2021-01-29 02:32:34</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39563</th>\n",
       "      <td>Dropbox plans to acquire DocSend for $165 million</td>\n",
       "      <td>4</td>\n",
       "      <td>m1bwis</td>\n",
       "      <td>https://www.marketwatch.com/story/dropbox-plan...</td>\n",
       "      <td>1</td>\n",
       "      <td>1.615341e+09</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2021-03-10 03:47:28</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34289</th>\n",
       "      <td>Am I doing this right? YOLO PLTR BITCHES!</td>\n",
       "      <td>82</td>\n",
       "      <td>lnei6t</td>\n",
       "      <td>https://www.reddit.com/gallery/lnei6t</td>\n",
       "      <td>33</td>\n",
       "      <td>1.613766e+09</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2021-02-19 22:16:19</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5520</th>\n",
       "      <td>I like $GME</td>\n",
       "      <td>0</td>\n",
       "      <td>l6zjl9</td>\n",
       "      <td>https://www.reddit.com/r/wallstreetbets/commen...</td>\n",
       "      <td>0</td>\n",
       "      <td>1.611876e+09</td>\n",
       "      <td>But I also like Dogecoin( $DOGE ).\\n\\nThat is ...</td>\n",
       "      <td>2021-01-29 01:23:23</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12121</th>\n",
       "      <td>Something is really suspicious here with robin...</td>\n",
       "      <td>1</td>\n",
       "      <td>l71r6d</td>\n",
       "      <td>https://www.reddit.com/r/wallstreetbets/commen...</td>\n",
       "      <td>8</td>\n",
       "      <td>1.611881e+09</td>\n",
       "      <td>So I have 2.3 stocks in GME via robinhood. As ...</td>\n",
       "      <td>2021-01-29 02:41:05</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>43100</th>\n",
       "      <td>RKT YOLO UPDATE. STILL HOLDING!</td>\n",
       "      <td>106</td>\n",
       "      <td>mbnahe</td>\n",
       "      <td>https://i.redd.it/8cpxv8cg2uo61.jpg</td>\n",
       "      <td>33</td>\n",
       "      <td>1.616558e+09</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2021-03-24 05:57:18</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>46980</th>\n",
       "      <td>Microsoft Directors Decided Bill Gates Needed ...</td>\n",
       "      <td>1449</td>\n",
       "      <td>ne0or5</td>\n",
       "      <td>https://www.wsj.com/articles/microsoft-directo...</td>\n",
       "      <td>499</td>\n",
       "      <td>1.621236e+09</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2021-05-17 10:15:28</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                   title  score      id  \\\n",
       "36207  I am seated on the AMC rocket awaiting take of...    137  lsqlri   \n",
       "25558  Unpinned Daily Discussion Thread for February ...      0  lcdbiy   \n",
       "4865   Go downvote Trading 212 on Play Store, Can't g...      2  l6zifm   \n",
       "11012  THE SQUEEZE HASN'T STARTED YET THIS IS JUST A ...      1  l71hrs   \n",
       "39563  Dropbox plans to acquire DocSend for $165 million      4  m1bwis   \n",
       "34289          Am I doing this right? YOLO PLTR BITCHES!     82  lnei6t   \n",
       "5520                                         I like $GME      0  l6zjl9   \n",
       "12121  Something is really suspicious here with robin...      1  l71r6d   \n",
       "43100                    RKT YOLO UPDATE. STILL HOLDING!    106  mbnahe   \n",
       "46980  Microsoft Directors Decided Bill Gates Needed ...   1449  ne0or5   \n",
       "\n",
       "                                                     url  comms_num  \\\n",
       "36207                https://i.redd.it/oltiuvk6arj61.jpg         42   \n",
       "25558  https://www.reddit.com/r/wallstreetbets/commen...       1501   \n",
       "4865   https://www.reddit.com/r/wallstreetbets/commen...          0   \n",
       "11012                https://i.redd.it/hnzd7uy9o3e61.jpg          8   \n",
       "39563  https://www.marketwatch.com/story/dropbox-plan...          1   \n",
       "34289              https://www.reddit.com/gallery/lnei6t         33   \n",
       "5520   https://www.reddit.com/r/wallstreetbets/commen...          0   \n",
       "12121  https://www.reddit.com/r/wallstreetbets/commen...          8   \n",
       "43100                https://i.redd.it/8cpxv8cg2uo61.jpg         33   \n",
       "46980  https://www.wsj.com/articles/microsoft-directo...        499   \n",
       "\n",
       "            created                                               body  \\\n",
       "36207  1.614345e+09                                                NaN   \n",
       "25558  1.612465e+09  Your daily trading discussion thread. Please k...   \n",
       "4865   1.611876e+09                                       Fuck them up   \n",
       "11012  1.611880e+09                                                NaN   \n",
       "39563  1.615341e+09                                                NaN   \n",
       "34289  1.613766e+09                                                NaN   \n",
       "5520   1.611876e+09  But I also like Dogecoin( $DOGE ).\\n\\nThat is ...   \n",
       "12121  1.611881e+09  So I have 2.3 stocks in GME via robinhood. As ...   \n",
       "43100  1.616558e+09                                                NaN   \n",
       "46980  1.621236e+09                                                NaN   \n",
       "\n",
       "                 timestamp  \n",
       "36207  2021-02-26 15:14:42  \n",
       "25558  2021-02-04 21:00:22  \n",
       "4865   2021-01-29 01:22:16  \n",
       "11012  2021-01-29 02:32:34  \n",
       "39563  2021-03-10 03:47:28  \n",
       "34289  2021-02-19 22:16:19  \n",
       "5520   2021-01-29 01:23:23  \n",
       "12121  2021-01-29 02:41:05  \n",
       "43100  2021-03-24 05:57:18  \n",
       "46980  2021-05-17 10:15:28  "
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Get Data\n",
    "train_text_file = keras.utils.get_file('train_text.txt', 'https://jrssbcrsefilesnait.blob.core.windows.net/3950data1/reddit_wsb.csv')\n",
    "train_text = pd.read_csv(train_text_file)\n",
    "train_text.sample(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "TOKENS = 500\n",
    "OUTPUT_LENGTH = 45\n",
    "OUT_DIM = 8\n",
    "BATCH_SIZE = 128\n",
    "SAMP = 7\n",
    "SHUFFLE = 500\n",
    "UNITS = 50"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Clean and Build Sequences\n",
    "\n",
    "We can clean and process the text data with any strategy. Here we'll use a simple strategy of removing punctuation and converting all text to lowercase.\n",
    "\n",
    "Next we'll use a Keras utility to convert our text into a sequence of words, which we'll then tokenize into integers. The sequence that is created is pretty simple, just a list of all the words in the sentence in the correct order. Basically an OCD version of a normal sentence. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['this',\n",
       " 'is',\n",
       " 'our',\n",
       " 'time',\n",
       " 'if',\n",
       " 'anyone',\n",
       " 'will',\n",
       " 'listen',\n",
       " 'to',\n",
       " 'you',\n",
       " 'please',\n",
       " 'explain',\n",
       " 'to',\n",
       " 'them',\n",
       " 'why',\n",
       " 'gme',\n",
       " 'is',\n",
       " 'one',\n",
       " 'of',\n",
       " 'the',\n",
       " 'very',\n",
       " 'best',\n",
       " 'investments',\n",
       " 'they',\n",
       " 'can',\n",
       " 'make',\n",
       " 'right',\n",
       " 'now',\n",
       " 'i',\n",
       " 'have',\n",
       " 'already',\n",
       " 'gotten',\n",
       " '5',\n",
       " 'people',\n",
       " 'close',\n",
       " 'to',\n",
       " 'me',\n",
       " 'to',\n",
       " 'research',\n",
       " 'what',\n",
       " 'is',\n",
       " 'happening',\n",
       " 'and',\n",
       " 'they',\n",
       " 'decided',\n",
       " 'to',\n",
       " 'dump',\n",
       " 'everything',\n",
       " 'into',\n",
       " 'gme',\n",
       " 'not',\n",
       " 'amc',\n",
       " 'not',\n",
       " 'nok',\n",
       " 'not',\n",
       " 'bb',\n",
       " 'but',\n",
       " 'gme',\n",
       " 'after',\n",
       " 'last',\n",
       " \"night's\",\n",
       " 'ah',\n",
       " 'manipulation',\n",
       " 'i',\n",
       " 'am',\n",
       " 'not',\n",
       " 'selling',\n",
       " 'until',\n",
       " 'at',\n",
       " 'least',\n",
       " '5k',\n",
       " 'to',\n",
       " '10k',\n",
       " 'a',\n",
       " 'share',\n",
       " 'we',\n",
       " 'need',\n",
       " 'to',\n",
       " 'encourage',\n",
       " 'people',\n",
       " 'to',\n",
       " 'have',\n",
       " 'the',\n",
       " 'courage',\n",
       " 'to',\n",
       " 'hold',\n",
       " 'we',\n",
       " 'finally',\n",
       " 'have',\n",
       " 'the',\n",
       " 'power',\n",
       " 'to',\n",
       " 'change',\n",
       " 'our',\n",
       " 'futures',\n",
       " 'they',\n",
       " 'want',\n",
       " 'us',\n",
       " 'begging',\n",
       " 'again',\n",
       " 'and',\n",
       " 'again',\n",
       " 'for',\n",
       " 'stimulus',\n",
       " 'checks',\n",
       " 'while',\n",
       " 'they',\n",
       " 'laugh',\n",
       " 'at',\n",
       " 'us',\n",
       " 'as',\n",
       " 'they',\n",
       " 'sip',\n",
       " 'champaign',\n",
       " 'from',\n",
       " 'gold',\n",
       " 'plated',\n",
       " 'glasses',\n",
       " 'we',\n",
       " 'are',\n",
       " 'a',\n",
       " 'fucking',\n",
       " 'joke',\n",
       " 'to',\n",
       " 'them',\n",
       " 'well',\n",
       " 'now',\n",
       " 'how',\n",
       " 'the',\n",
       " 'fuck',\n",
       " 'does',\n",
       " 'it',\n",
       " 'feel',\n",
       " 'this',\n",
       " 'is',\n",
       " 'our',\n",
       " 'chance',\n",
       " 'at',\n",
       " 'financial',\n",
       " 'freedom',\n",
       " 'this',\n",
       " 'is',\n",
       " 'our',\n",
       " 'moment',\n",
       " 'fellow',\n",
       " 'poors',\n",
       " 'rise',\n",
       " 'up',\n",
       " 'we',\n",
       " 'will',\n",
       " 'be',\n",
       " 'enslaved',\n",
       " 'no',\n",
       " 'more']"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "raw_text = train_text['body']\n",
    "## Remove punctuation\n",
    "raw_text = raw_text.dropna()\n",
    "raw_text = raw_text.apply(lambda x: x.replace('[{}]'.format(string.punctuation), '').lower())\n",
    "vocab = set()\n",
    "sentences = []\n",
    "for sentence in raw_text:\n",
    "  current_sentence = text_to_word_sequence(sentence)\n",
    "  sentences.append(current_sentence)\n",
    "  vocab.update(current_sentence)\n",
    "#vocab\n",
    "#sentences\n",
    "max_length = max([len(sentence) for sentence in sentences])\n",
    "sentences[SAMP]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Tokenize\n",
    "\n",
    "We can take the text that we split above and encode it as a sequence of integers. We'll then use the tokenizer to convert our sequences into a sequence of integers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[9,\n",
       " 6,\n",
       " 131,\n",
       " 78,\n",
       " 28,\n",
       " 380,\n",
       " 25,\n",
       " 2,\n",
       " 14,\n",
       " 305,\n",
       " 2,\n",
       " 76,\n",
       " 128,\n",
       " 43,\n",
       " 6,\n",
       " 73,\n",
       " 5,\n",
       " 1,\n",
       " 165,\n",
       " 138,\n",
       " 17,\n",
       " 40,\n",
       " 99,\n",
       " 145,\n",
       " 57,\n",
       " 8,\n",
       " 20,\n",
       " 229,\n",
       " 112,\n",
       " 69,\n",
       " 317,\n",
       " 2,\n",
       " 89,\n",
       " 2,\n",
       " 452,\n",
       " 44,\n",
       " 6,\n",
       " 3,\n",
       " 17,\n",
       " 2,\n",
       " 421,\n",
       " 96,\n",
       " 43,\n",
       " 23,\n",
       " 175,\n",
       " 23,\n",
       " 23,\n",
       " 374,\n",
       " 24,\n",
       " 43,\n",
       " 136,\n",
       " 126,\n",
       " 8,\n",
       " 135,\n",
       " 23,\n",
       " 228,\n",
       " 308,\n",
       " 22,\n",
       " 418,\n",
       " 2,\n",
       " 4,\n",
       " 113,\n",
       " 26,\n",
       " 173,\n",
       " 2,\n",
       " 69,\n",
       " 2,\n",
       " 20,\n",
       " 1,\n",
       " 2,\n",
       " 134,\n",
       " 26,\n",
       " 20,\n",
       " 1,\n",
       " 429,\n",
       " 2,\n",
       " 391,\n",
       " 131,\n",
       " 17,\n",
       " 143,\n",
       " 82,\n",
       " 232,\n",
       " 3,\n",
       " 232,\n",
       " 12,\n",
       " 188,\n",
       " 17,\n",
       " 22,\n",
       " 82,\n",
       " 21,\n",
       " 17,\n",
       " 31,\n",
       " 26,\n",
       " 15,\n",
       " 4,\n",
       " 177,\n",
       " 2,\n",
       " 76,\n",
       " 149,\n",
       " 57,\n",
       " 92,\n",
       " 1,\n",
       " 222,\n",
       " 248,\n",
       " 11,\n",
       " 436,\n",
       " 9,\n",
       " 6,\n",
       " 131,\n",
       " 22,\n",
       " 139,\n",
       " 9,\n",
       " 6,\n",
       " 131,\n",
       " 38,\n",
       " 26,\n",
       " 25,\n",
       " 19,\n",
       " 79,\n",
       " 42]"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer = Tokenizer(num_words=TOKENS)\n",
    "tokenizer.fit_on_texts(sentences)\n",
    "\n",
    "tokenized_sentences = tokenizer.texts_to_sequences(sentences)\n",
    "tokenized_sentences[SAMP]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Construct Training Sequences\n",
    "\n",
    "We are going to be a little slack in the construction of the datasets for training because we are limited in the amount of resources we can handle. All of the datasets that are fed to the model need to be the same length, so we'll set a cap and trucate it here for resource concerns. Our dataset will be constructed as:\n",
    "<ul>\n",
    "<li> A sequence of (up to) 24 words as the X data. \n",
    "<li> The next word as the Y data.\n",
    "</ul>\n",
    "\n",
    "So each sequence is effectively one set of features, and its target is the next word. If we were doing this in reality, we'd want to prep more records from our sample:\n",
    "<ul>\n",
    "<li> Suppose a sample sentence is \"The quick brown fox jumps over the lazy dog\". Our ideal data would have something like:\n",
    "    <ul> \n",
    "    <li>X = \"the quick brown\", Y = \"fox\"\n",
    "    <li> X = \"quick brown fox\", Y = \"jumps\"\n",
    "    <li> X = \"brown fox jumps\", Y = \"over\"\n",
    "    </ul>\n",
    "</ul>\n",
    "\n",
    "This is superior both because we are generating much more data to train the model and because we are training the model to predict words in all different positions in the sentence. We'd be predicting almost every word in the training dataset. The words that frequently end a sentence are not necessarily the same as the words that start a sentence or sit in the middle, so making predictions up and down the text will likely lead to a more useful model.\n",
    "\n",
    "<b>We'd end up with a better model if we generated more sequences from our data and/or added more data. The resource demands make that tough, so we have cut some corners that are easy to remedy in a real-world scenario.</b>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Longest real sequence: 4572\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[9,\n",
       " 6,\n",
       " 131,\n",
       " 78,\n",
       " 28,\n",
       " 380,\n",
       " 25,\n",
       " 2,\n",
       " 14,\n",
       " 305,\n",
       " 2,\n",
       " 76,\n",
       " 128,\n",
       " 43,\n",
       " 6,\n",
       " 73,\n",
       " 5,\n",
       " 1,\n",
       " 165,\n",
       " 138,\n",
       " 17,\n",
       " 40,\n",
       " 99,\n",
       " 145,\n",
       " 57,\n",
       " 8,\n",
       " 20,\n",
       " 229,\n",
       " 112,\n",
       " 69,\n",
       " 317,\n",
       " 2,\n",
       " 89,\n",
       " 2,\n",
       " 452,\n",
       " 44,\n",
       " 6,\n",
       " 3,\n",
       " 17,\n",
       " 2,\n",
       " 421,\n",
       " 96,\n",
       " 43,\n",
       " 23,\n",
       " 175]"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Find longest sequence\n",
    "max_real_length = max([len(sentence) for sentence in tokenized_sentences])\n",
    "print(\"Longest real sequence:\", max_real_length)\n",
    "if max_real_length > OUTPUT_LENGTH:\n",
    "  trunc_token_sequences = [t_[:OUTPUT_LENGTH] for t_ in tokenized_sentences]\n",
    "trunc_token_sequences[SAMP]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Pad Sequences\n",
    "\n",
    "We need to pad our sequences so that they are all the same length, as our neural networks require that. The pad_sequences utility does just that, it will fill 0s at either the beginning or end of the sequence, depending on the \"padding\" option, and make everything the same length. We want to pad before the real data, because we are always planning on predicting the next token, so we want to work from the last value. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([  9,   6, 131,  78,  28, 380,  25,   2,  14, 305,   2,  76, 128,\n",
       "        43,   6,  73,   5,   1, 165, 138,  17,  40,  99, 145,  57,   8,\n",
       "        20, 229, 112,  69, 317,   2,  89,   2, 452,  44,   6,   3,  17,\n",
       "         2, 421,  96,  43,  23, 175], dtype=int32)"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "padded_trunc_token_sequences = pad_sequences(trunc_token_sequences, maxlen=OUTPUT_LENGTH, padding='pre')\n",
    "padded_trunc_token_sequences[SAMP]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Write Tokenized Data to Disk\n",
    "\n",
    "We can write the tokenized data to disk so that we can use it later. This will save us from having to redo that step that is slow. Since we chopped our data size down a bit, this isn't super needed. I tried to generate all of the sequences noted above and I both exceeded the Colab memory limits and it took a while to run. I wouldn't want to repeat that if I can avoid it, and hard drive space is cheap, so writing the interim results to disk is a good work around. In real scenarios when we had massive amounts of data we would need to load it incrementally from disk anyway, so this is a free win."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "#token_path = 'data/sample_tokenized_sentences.csv'\n",
    "token_path = 'data/padded_sample_tokenized_sentences.csv'\n",
    "if IN_COLAB:\n",
    "  !mkdir data\n",
    "  \n",
    "if os.path.exists(token_path):\n",
    "    df_prepped = pd.read_csv(token_path, header=None)\n",
    "    prepped_sentences = np.array(df_prepped.values.tolist())\n",
    "else:\n",
    "    df_prepped = pd.DataFrame(padded_trunc_token_sequences)\n",
    "    df_prepped.to_csv(token_path, header=False, index=False)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Dataset and Array\n",
    "\n",
    "This data can be reasonably used either as a regular array or or a tensorflow dataset. We'll use both here, just to show that it can be done either way. If we we using the full set of sequences the data would be quite a bit larger with all the fully padded out sequences, we'd probably need to write the data to disk and load it with a dataset or generator. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(24738, 45)\n",
      "Y shape: (24738, 500)\n",
      "X shape: (24738, 44)\n"
     ]
    }
   ],
   "source": [
    "print(padded_trunc_token_sequences.shape)\n",
    "y_t = padded_trunc_token_sequences[:, -1].reshape(-1, 1)\n",
    "y_t = ku.to_categorical(y_t, num_classes=TOKENS)\n",
    "print(\"Y shape:\", y_t.shape)\n",
    "X_t = padded_trunc_token_sequences[:, :-1]\n",
    "print(\"X shape:\", X_t.shape)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model\n",
    "\n",
    "Now we model. The data that we made mirrors the construction of a sentence.\n",
    "<ul>\n",
    "<li> X features - the sentence up to this point. \n",
    "<li> Y target - the word(s) that should come next. \n",
    "</ul>\n",
    "\n",
    "So, the model is effectively working to generate text just like a time series model works to predict the next value in a sequence of stock prices or hourly temperature. We train the model on, hopefully a large number of senteneces, where is sees many examples of \"here are some words\" (X values) and \"here is the next word\" (Y value). If we give it lots and lots of that training data, it should become better and better at determining what should come next, given the existing sentence. \n",
    "\n",
    "To do this well, we'd need a lot more data than we have, and much more time to train. We'd want to give the model enough data so that it can see lots and lots of examples of the same word in different contexts, and of similar contexts with different words. The patterns of language are really complex, so we need data that provides enough variation to demonstrate the patterns. \n",
    "\n",
    "The model is wrapped in a little function, so we can make a model to output a different number of words with more convenience.\n",
    "\n",
    "#### Embedding Layer\n",
    "\n",
    "We also use an embedding layer here, which accepts our integer inputs and converts them to a vector of a specified size. This is a way of representing the words in a way that is more useful for the model. We saw embeddings with word2vec during the NLP portions. Just as it did then, embedding will represent each of our tokens as a vector of a specified size, or as a value in N dimensional space. \n",
    "\n",
    "![Embedding](images/embedding.png \"Embedding\")\n",
    "\n",
    "When we check the summary of the model we can see that the embedding layer has a lot of parameters, this is because it is learning the vector representation of each of the words in our vocabulary. When we used word2vec we used a pretrained model - the N dimensional space was already defined, and we placed our words into it. Here, we are letting the model learn the space and the vectors that represent the words, so the vector representation of each word is learned directly from the data. If we are dealing with a scenarion where we have a lot of data and a specialized vocabulary (such as in industry) this can be very useful. The model will learn which words are similar and which are not, based on the context of the text provided in training.\n",
    "\n",
    "#### Let's Go Bi\n",
    "\n",
    "For this model we'll use a bidirectional LSTM layer. This is a layer that will process the input sequence in both directions, so it will see the sequence from the beginning and from the end. This is useful because it allows the model to consider what should come next, but also what should come before, which can be useful for determining the next word.\n",
    "\n",
    "![Bidirectional LSTM](images/bidirectional.webp \"Bidirectional LSTM\")\n",
    "\n",
    "Bidirectional layers are most prevalent in NLP, as adding the ability to look at the sequence of words in both directions can really help models build a better understanding of the context of the words. There is a speed penalty due to each layer doing roughly double the number of calculations, but for language tasks such as this, it is pretty likely to be worth it, particularly if we were to expand the dataset. Bidirectional layers can be added simply, as below; on the Keras documentation there are a few more details, mainly that the forward and backward layers can be configured separately then combined. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_1\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " embedding_1 (Embedding)     (None, 44, 8)             4000      \n",
      "                                                                 \n",
      " bidirectional_4 (Bidirectio  (None, 44, 100)          23600     \n",
      " nal)                                                            \n",
      "                                                                 \n",
      " bidirectional_5 (Bidirectio  (None, 44, 100)          60400     \n",
      " nal)                                                            \n",
      "                                                                 \n",
      " bidirectional_6 (Bidirectio  (None, 100)              60400     \n",
      " nal)                                                            \n",
      "                                                                 \n",
      " flatten_1 (Flatten)         (None, 100)               0         \n",
      "                                                                 \n",
      " dense_1 (Dense)             (None, 500)               50500     \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 198,900\n",
      "Trainable params: 198,900\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "model = Sequential()\n",
    "model.add(Embedding(input_dim=TOKENS, output_dim=OUT_DIM, input_length=OUTPUT_LENGTH-1))\n",
    "#model.add(LSTM(UNITS, return_sequences=True))\n",
    "#model.add(LSTM(UNITS, return_sequences=True))\n",
    "#model.add(LSTM(UNITS))\n",
    "model.add(Bidirectional(LSTM(UNITS, return_sequences=True)))\n",
    "model.add(Bidirectional(LSTM(UNITS, return_sequences=True)))\n",
    "model.add(Bidirectional(LSTM(UNITS)))\n",
    "model.add(Flatten())\n",
    "model.add(Dense(TOKENS, activation='softmax'))\n",
    "\n",
    "model.compile(optimizer='rmsprop',loss='categorical_crossentropy',metrics=['acc'])\n",
    "print(model.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Some larger values for Colab\n",
    "TEST_EPOCHS = 10\n",
    "TEST_BATCH = 512\n",
    "\n",
    "if IN_COLAB:\n",
    "    TEST_EPOCHS = 300\n",
    "    TEST_BATCH = 1024"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "49/49 [==============================] - 33s 497ms/step - loss: 5.6863 - acc: 0.0371\n",
      "Epoch 2/5\n",
      "49/49 [==============================] - 29s 589ms/step - loss: 5.3853 - acc: 0.0538\n",
      "Epoch 3/5\n",
      "49/49 [==============================] - 26s 539ms/step - loss: 5.3809 - acc: 0.0538\n",
      "Epoch 4/5\n",
      "49/49 [==============================] - 26s 538ms/step - loss: 5.3799 - acc: 0.0538\n",
      "Epoch 5/5\n",
      "49/49 [==============================] - 27s 543ms/step - loss: 5.3785 - acc: 0.0538\n"
     ]
    }
   ],
   "source": [
    "# Try with dataframes\n",
    "early_stop = EarlyStopping(monitor='loss', patience=50)\n",
    "tensorboard_callback = tf.keras.callbacks.TensorBoard(log_dir=\"logs\")\n",
    "save_weights = tf.keras.callbacks.ModelCheckpoint('weights/lstm_gen_weights.h5', save_best_only=True, monitor='loss', mode='min', save_weights_only=True)\n",
    "\n",
    "if IN_COLAB:\n",
    "    !mkdir logs\n",
    "    !mkdir weights\n",
    "    %load_ext tensorboard\n",
    "    %tensorboard --logdir logs\n",
    "\n",
    "history = model.fit(X_t, y_t, batch_size=TEST_BATCH, epochs=TEST_EPOCHS, verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_ds = tf.data.Dataset.from_tensor_slices((X_t, y_t)).shuffle(SHUFFLE).batch(TEST_BATCH).prefetch(tf.data.experimental.AUTOTUNE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "49/49 [==============================] - 28s 554ms/step - loss: 5.3921 - acc: 0.0538\n",
      "Epoch 2/5\n",
      "49/49 [==============================] - 28s 567ms/step - loss: 5.4334 - acc: 0.0538\n",
      "Epoch 3/5\n",
      "49/49 [==============================] - 25s 516ms/step - loss: 5.4142 - acc: 0.0538\n",
      "Epoch 4/5\n",
      "49/49 [==============================] - 26s 534ms/step - loss: 5.4069 - acc: 0.0538\n",
      "Epoch 5/5\n",
      "49/49 [==============================] - 26s 530ms/step - loss: 5.3859 - acc: 0.0563\n"
     ]
    }
   ],
   "source": [
    "# Or with datasets...\n",
    "history = model.fit(train_ds, epochs=TEST_EPOCHS, verbose=1, callbacks=[early_stop, tensorboard_callback, save_weights])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generate Some Text\n",
    "\n",
    "We can generate text once the model is trained. We'll start with a seed sentence, and then we'll use the model to predict the next word. We'll then append that word to the sentence and use the model to predict the next word, and so on. There is a little helper function to do this for us with limited repetition.\n",
    "\n",
    "Another new thing is that we create an inverse dictionary to map the encoded words back to the original words.\n",
    "\n",
    "### Temperature\n",
    "\n",
    "One weirdly named factor that is important in text generation is the temperature. The temperature is a factor that we can use to control the randomness of the output. To generate text, we are essentially using a probability distribution to determine what the next word should be - the softmax output of the model will tell us the most likely next word. The issue is that certain words are way more likely than others - \"the\", \"it\", \"a\", \"and\", etc. are all very common words so we can expect the model to predict them as \"most likely\" a lot, probably too often. \n",
    "\n",
    "![Temperature](images/temperature.gif \"Temperature\")\n",
    "\n",
    "The most direct way to combat this is to add some degree of randomness to which word we select - we'll still pick the most likely word more often than any other, but we'll also pick other words that have some degree of likelihood at random. The higher the temperature the more randomness is introduced. A correct value requires tuning with human feedback, and it'll vary depending on the base quality of the model - large models that are trained on huge volumes of text and are deep enough to pick up on the \"what type of word should be here\" patterns will be able to generate better text with lower temperatures. Our model here is small and kind of sucks, so the temperature needs to be higher to get anything remotely usable. The implementation here is stolen shamelessly from the internet, the details don't really matter all that much, we just need to vary our predictions away from always simply picking the most likely word."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "inverse_dict = {v: k for k, v in tokenizer.word_index.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "from random import *\n",
    "\n",
    "def sampleWord(preds, temperature=1.0):\n",
    "    preds = np.asarray(preds).astype('float64')\n",
    "    preds = np.log(preds) / temperature\n",
    "    exp_preds = np.exp(preds)\n",
    "    preds = exp_preds / np.sum(exp_preds)\n",
    "    probas = np.random.multinomial(1, preds.flatten(), 1)\n",
    "    return np.argmax(probas)\n",
    "\n",
    "def generate_text(seed_text, next_words, model, length_max, tok, inverse, temperature=1.0):\n",
    "    out = seed_text\n",
    "    for _ in range(next_words):\n",
    "        token_list = tok.texts_to_sequences([out])[0]\n",
    "        token_list = pad_sequences([token_list], maxlen=length_max-1, padding='pre')\n",
    "        #print(token_list)\n",
    "        predicted = model.predict(token_list, verbose=0)\n",
    "        #print(tok.sequences_to_texts(predicted))\n",
    "        word = inverse[sampleWord(predicted, temperature=temperature)]\n",
    "        out += \" \"+word\n",
    "        #print(out)\n",
    "    return out"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Fake text time!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Last night I went on a date with not the days the you and strong of can fuck i’m do will there to me on know value it way almost not its a guys comments x200b b 3 \n",
      "\n",
      "We are going to the  them anything they i as new options money the 1 change and width not and already is some auto retards weeks have life for my then chart and apes and \n",
      "\n",
      "Where are all of the apes it to its in both up a on but now over why low potential auto money really pretty if done at total know his with and shit a market \n",
      "\n",
      "I am scared have of stock t it and take as a holding was 0 great a people t well lot help a moon be s gme no stocks keep been time to \n",
      "\n",
      "School is out for bad to that in format end r what stock big time by covid i 3 that the a here after in time of can't day shorts be end and s \n",
      "\n"
     ]
    }
   ],
   "source": [
    "print (generate_text(\"Last night I went on a date with\", next_words=30, model=model, length_max=OUTPUT_LENGTH, tok=tokenizer, inverse=inverse_dict), \"\\n\")\n",
    "print (generate_text(\"We are going to the \", next_words=30, model=model, length_max=OUTPUT_LENGTH, tok=tokenizer, inverse=inverse_dict), \"\\n\")\n",
    "print (generate_text(\"Where are all of the\", next_words=30, model=model, length_max=OUTPUT_LENGTH, tok=tokenizer, inverse=inverse_dict), \"\\n\")\n",
    "print (generate_text(\"I am scared\", next_words=30, model=model, length_max=OUTPUT_LENGTH, tok=tokenizer, inverse=inverse_dict), \"\\n\")\n",
    "print (generate_text(\"School is out for\", next_words=30, model=model, length_max=OUTPUT_LENGTH, tok=tokenizer, inverse=inverse_dict), \"\\n\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A few long ones..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Last night I went on a date with go their like the the months until than 2021 100 term on i in the the or the today may put platform in with however s and was why are i 15 https a still hold no at hold it so term advice if a them please 2 my today investor hedge great huge all though t t the webp s 4 moon may right back still could profit and back strong the the lose put trading 8 there png good that's 13 every 5 i'm go high in is here public t after selling on 13 am the use \n",
      "\n"
     ]
    },
    {
     "ename": "KeyError",
     "evalue": "0",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[0;32m/var/folders/px/vhm_920n7zx2wvqq_ht0q5tm0000gp/T/ipykernel_5990/2819587574.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mprint\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mgenerate_text\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Last night I went on a date with\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnext_words\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m100\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlength_max\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mOUTPUT_LENGTH\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtok\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtokenizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minverse\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minverse_dict\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"\\n\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mprint\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mgenerate_text\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"My toe is purple and I am\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnext_words\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m100\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlength_max\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mOUTPUT_LENGTH\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtok\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtokenizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minverse\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minverse_dict\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"\\n\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0mprint\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mgenerate_text\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"The Toronto Raptors won and Rob Ford's body rose from the dead\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnext_words\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m100\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlength_max\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mOUTPUT_LENGTH\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtok\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtokenizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minverse\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minverse_dict\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"\\n\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/var/folders/px/vhm_920n7zx2wvqq_ht0q5tm0000gp/T/ipykernel_5990/342767763.py\u001b[0m in \u001b[0;36mgenerate_text\u001b[0;34m(seed_text, next_words, model, length_max, tok, inverse, temperature)\u001b[0m\n\u001b[1;32m     17\u001b[0m         \u001b[0mpredicted\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtoken_list\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mverbose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m         \u001b[0;31m#print(tok.sequences_to_texts(predicted))\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 19\u001b[0;31m         \u001b[0mword\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0minverse\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0msampleWord\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpredicted\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtemperature\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtemperature\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     20\u001b[0m         \u001b[0mout\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;34m\" \"\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0mword\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     21\u001b[0m         \u001b[0;31m#print(out)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyError\u001b[0m: 0"
     ]
    }
   ],
   "source": [
    "print (generate_text(\"Last night I went on a date with\", next_words=100, model=model, length_max=OUTPUT_LENGTH, tok=tokenizer, inverse=inverse_dict), \"\\n\")\n",
    "print (generate_text(\"My toe is purple and I am\", next_words=100, model=model, length_max=OUTPUT_LENGTH, tok=tokenizer, inverse=inverse_dict), \"\\n\")\n",
    "print (generate_text(\"The Toronto Raptors won and Rob Ford's body rose from the dead\", next_words=100, model=model, length_max=OUTPUT_LENGTH, tok=tokenizer, inverse=inverse_dict), \"\\n\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And if we make the temperature low, we likely get worse results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Last night I went on a date with gme the moon stock stock gme the moon stock stock stock the moon stock stock gme gme gme of the of the of the of the of the of the \n",
      "\n",
      "We are going to the  moon moon gme stock stock stock stock the stock stock the moon gme gme the moon stock stock stock of the of the of the of the of the of \n",
      "\n"
     ]
    }
   ],
   "source": [
    "print (generate_text(\"Last night I went on a date with\", next_words=30, model=model, length_max=OUTPUT_LENGTH, tok=tokenizer, inverse=inverse_dict, temperature=.1), \"\\n\")\n",
    "print (generate_text(\"We are going to the \", next_words=30, model=model, length_max=OUTPUT_LENGTH, tok=tokenizer, inverse=inverse_dict, temperature=.1), \"\\n\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusion and Notes\n",
    "\n",
    "The LSTM model works well for text generation, if we had more data, more time, and spent a bit more effort cleaning up the edge cases in our code here we could likely get something pretty legible. \n",
    "\n",
    "The biggest difference between this and the impressive larger models is that they are higher capacity, trained with more data, and sometimes tuned with human feedback. These things combine to make the model more likely to pick up on the patterns of language and generate text that is more likely to be grammatically correct and follow the expected flow of language. Our model is really struggling to just find a reasonable word to use next, we haven't allowed it to learn enough to really construct logical sentences."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ml3950",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "4d722d3adfa415172c1f5238b519fb86b488acdae450fd691ab06c09f4ca9173"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
