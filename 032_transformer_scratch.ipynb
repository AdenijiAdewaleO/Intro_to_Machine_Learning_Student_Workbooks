{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-04-19 11:31:32.900850: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "from tensorflow.keras.layers import TextVectorization\n",
    "import numpy as np\n",
    "import os\n",
    "import re\n",
    "import string\n",
    "import random\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# WhackGPT\n",
    "\n",
    "We can make a transformer based model to generate chatGPT-ish text responses. Ours will be far more stupid, but hey, it's a taking computer. \n",
    "\n",
    "## Transformer Architecture\n",
    "\n",
    "What the heck is a transformer, what does it do, and why is it so cool? A transformer model is a type of neural network that was creating in 2017 at Google. The core idea behind transformers is the idea of attention, which is deailed a little bit below. The diagramed strucutre of a transformer model can be a little intimidating, but we can make sense of the critical parts without too much issue. \n",
    "\n",
    "![Transformer](images/transformer.png \"Transformer\")\n",
    "\n",
    "A transformer model contains a few key parts, each of with is dealt with in more detail below.\n",
    "<ul>\n",
    "<li> Embedding - the embedding layer generates embeddings (vector representations) for each token. The embeddings are created for both the token itself and its position in the sequence. </li>\n",
    "<li> Attention layers - the attention layers are the core of the transformer model. They are responsible for creating a representation of the input sequence that is used to generate the output sequence. </li>\n",
    "<li> Encoder - the encoder is a stack of attention layers that are used to create a representation of the input sequence. </li>\n",
    "<li> Decoder - the decoder is a stack of attention layers that are used to create a representation of the output sequence. </li>\n",
    "</ul>\n",
    "\n",
    "The attention part is the star of the show, it is a method to be able to focus the attention of the model on the critical portions of the input sequence and generate contextually informed predictions for the output. As well, transformers do all of this in a way that is more parallelizable than LSTM based models that were the state of the art before transformers, only a few years ago. "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Embedding\n",
    "\n",
    "The embedding here has two parts:\n",
    "<ul>\n",
    "<li> Token embedding: This maps each token to a vector representation in N-dimensional space. This is what we are used to for embedding. The original transformer paper used a 512-dimensional embedding, so each token was represented by a vector of 512 values that position it on a 512D grid. \n",
    "<li> Positional embedding: This maps each token's position in the <i>sequence</i>. The position embedding can be thought of as an extension of the concept of just tracking which word of a sentence each token is, 1,2,3...\n",
    "</ul>\n",
    "\n",
    "#### Token Embedding\n",
    "\n",
    "Token embedding is something that we are used to from when we used word2vec to generate embeddings for classification models. We are tranlating each token into an N-dimensional representation in space. The big difference here is that our embedding space is being learned by the model during training, so we should expect that the model will be shifting each token around in space as it learns more about what that word means, or more accurately, how it is used in our training data. \n",
    "\n",
    "![Embedding](images/embedding.png \"Embedding\")\n",
    "\n",
    "#### Positional Embedding\n",
    "\n",
    "The positional embedding is needed and most clearly seen if we compare this to an LSTM. In an LSTM, the position of a token is always known as we process the data sequentially. In the transformer model, the data is taken in parallel, so we don't have the sequence data built in. This has the benefit of allowing the model to process more of its work in parallel than an LSTM, but it also means that the model needs to be told where each token is in the sequence. What is the positional embedding? It follows the same concept as the token embedding, we are representing something with a vector of values. In the positional embedding, the math is a little involved, but it uses sine and cosine functions to represent the position of a token. \n",
    "\n",
    "![Positional Embedding](images/positional_emb.png \"Positional Embedding\")\n",
    "\n",
    "Where:\n",
    "<ul>\n",
    "<li> <b>k:</b> position of the token. \n",
    "<li> <b>d:</b> dimension of the embedding.\n",
    "<li> <b>i:</b> used for mapping to both sine and cosine functions.\n",
    "</ul>\n",
    "\n",
    "This positional embedding uses the trig functions to introduce some additional capability to our embedding values. First, this helps if we encounter longer sentences later on - if we embedded the position with a simple word count number, that would be an issue for us. Second, the trig functions allow us to embed the position in a way that is not deterministic. This means that the model can learn where tokens occur in relation to each other without being told explicitly. This is useful if you think of sentences such as:\n",
    "<ul>\n",
    "<li> I do not like the story of the movie, but I do like the cast.\n",
    "<li> I do like the story of the movie, but I do not like the cast.\n",
    "</ul>\n",
    "\n",
    "These two sentences use the same words, but the meaning is opposite. The positional embedding helps capture the relationship between the words based on where the occur, and connect words that occur in certain \"areas\" to those in other \"areas\" of a sentence. This is really useful if you think of something like an adjective, that adjective modifies some noun, and understanding English requires that we are able to identify which noun it belongs to. Positional embedding with sine/cosine help with that, the position is recorded not only in a way that tells us where a word sits in an absolute sense, but it tells us where that word sits relative to the other words it is with. This is one reason transformers are so useful for tasks like language, their ability to contextualize the relationships in parts of text surpasses that of other models that we have today; when generating text, this gives us the most natural sounding text, as the \"next word\" prediction is based on a more comprehensive understanding of the sentence. \n",
    "\n",
    "Notably, the positional embedding uses the word embedding dimension, d, as the dimension of the positional embedding. This is because the positional embedding is added to the token embedding, so the two need to be the same dimension. This means that the embedding matrix generated can be quite large for each token. This also means that the input to any future modelling is going to contain those two vectors, likely represented in a high dimension - what is the token, and where is it in the sequence."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TokenAndPositionEmbedding(layers.Layer):\n",
    "    def __init__(self, maxlen, vocab_size, embed_dim):\n",
    "        super().__init__()\n",
    "        self.token_emb = layers.Embedding(input_dim=vocab_size, output_dim=embed_dim)\n",
    "        self.pos_emb = layers.Embedding(input_dim=maxlen, output_dim=embed_dim)\n",
    "\n",
    "    def call(self, x):\n",
    "        maxlen = tf.shape(x)[-1]\n",
    "        positions = tf.range(start=0, limit=maxlen, delta=1)\n",
    "        positions = self.pos_emb(positions)\n",
    "        x = self.token_emb(x)\n",
    "        return x + positions"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Transformer Construction\n",
    "\n",
    "We can now create a function to construct the core piece of our model, the transformer. The transformer layer has a few parts, the critical one being the attention layer. \n",
    "\n",
    "<b>Note:</b> the declarations of the layers are slightly different in a functional model. Each layer is a function that takes an input tensor and returns an output tensor. The layers are then called in the call method of the model.\n",
    "\n",
    "### Pay Attention\n",
    "\n",
    "The core piece of the transformer architecture is the attention mechanism. Attention serves as a way to focus, or pay attention, to certain parts of the input. The original paper that outlined the transformer architecture, \"Attention is All You Need\" from 2017 outlined the concept of attention at a high level - the idea is that we can use attention to focus on the most important parts of the input sequence, and use that to generate the output sequence. This is what humans do when reading something, we understand that certain parts of text add context allowing us to understand other parts. Attention is a way to teach a model to do the same thing. In RNN models, the data is always processed as a sequence - this has the advantage of allowing us to always know the order, but it slows computation and prevents the model from excelling at building relationships between parts of the data that are far from each other in the sequence. The transformer architecture replaces sequential processing of a sequence of values with parallel processing of the entire sequence at once, and build connections between the current part we are working on (e.g. the next word to generate) and all other parts of the sequence, focusing our attention on the most important parts.\n",
    "\n",
    "![Attention](images/attention.png \"Attention\")\n",
    "\n",
    "The result of that can be illustrated in the diagram above, with some additional labels. Here we have an illustration of a sentence, focusing on the last word, with colored and shaded lines indicating how much \"attention\" that word (token) needs to pay to the other tokens. Phrased differently, the stronger the line, the more that token on the left influences what the last token on the right should be. With training and lots of data, the model can eventually learn which other parts of a sequence the current token should \"pay attention to\", and use that to generate the next token in the sequence. Given enough training, this goes beyond words, and the model will begin to understand the relationships between types of words and parts of sentences. As a noticable example, words such \"it\", \"in\", or \"is\" can be used in many different contexts, and often refer to different nouns or verbs in a setence. The models can begin to decipher that certain types of words come after \"is\", and others come after \"in\". In this example, the model is paying strong attention to \"in\" and \"European\", since the model has learned enough that \"in\" indicates that the next word is some variety of place, and \"European\" indicates that said place is likely somewhere in Europe (the self-attention can be ignored). As models learn, see more language, and see sequences that are structured differently, the model will begin to understand more about the relationships between words and parts of sentences, and use that to generate sentences that not only use the correct words, but are structured in a way that mirrors the training data that it has seen. This sometimes gives the appearance that the model is thinking up and generating these answers on the fly, based on what it knows, but the language models don't really \"know\" anything, in an epistemological sense, they are just excellent at subtle pattern recognition in language. \n",
    "\n",
    "#### Attention Implementation\n",
    "\n",
    "The attention mechanism contains three key matrices that we'll ultimately use to calculate things:\n",
    "<ul>\n",
    "<li> Query\n",
    "<li> Key\n",
    "<li> Value\n",
    "</ul>\n",
    "\n",
    "The query, key, values are commonly described as analagous to doing a Google search. For example, when you search for videos on Youtube, the search engine will map your <b>query</b> (text in the search bar) against a set of <b>keys</b> (video title, description, etc.) associated with candidate videos in their database, then present you the best matched <b>values</b> (videos).  \n",
    "\n",
    "Using the query, key, and value objects involves a multistep process. \n",
    "<ul>\n",
    "<li> First, the query, key, and value all get a copy of the embedding (position and token) matrix fed in, which is then multipled by a set of weights that belong to a linear layer (no activation) for that Q/K/V input. \n",
    "<li> The value matrix is set aside for the moment. \n",
    "<li> The results of the query and key matricies are then mutipled by each other, which generates attention scores. \n",
    "<li> The result is then passed through a softmax function to normalize the weights and generates the actual attention mask. \n",
    "<li> The normalized weights are then multiplied by the value matrix, which gives us the final output.\n",
    "</ul>\n",
    "\n",
    "To ultimately create the layer, we have several of these heads, similar to filters in a CNN. \n",
    "\n",
    "![Multi-Head Attention](images/multi_head_att.png \"Multi-Head Attention\")\n",
    "\n",
    "Take and example of a sentence being, \"“Anthony Hopkins admired Michael Bay as a great director\", the product of the query and key matricies would look something like this:\n",
    "\n",
    "![Attention Mechanism](images/key_value.png \"Attention Mechanism\")\n",
    "\n",
    "These attention scores are measures of how important each word in the input sequence is to each other word. We normally see each word being really important to itself, then as the similarity decreases, the importance decreases. In this example, \"Hopkins\" and \"Anthony\" have a high score of attention with respect to each other, which makes sense! We would likely want to produce those two words in sequence. Given large amounts of data, the model can become very good at identifying what is important and what is not, and in particular, understanding context. Because the attention is based on the positon and token embeddings, and we have multiple heads (see below) each honing in on some other aspect of the text, the model can learn relationships between parts of speech that are challenging for other types of models, such as a sentence that has a lage independent clause in the middle of it or figures of speach that have little impact on the meaning of a sentence. Importantly, each token in a sentence is taken as the input, so we generate such a matrix for each \"query\" token.\n",
    "\n",
    "![Attention Sequences](images/attention_seq.png \"Attention Sequences\")\n",
    "\n",
    "#### Attention Masking\n",
    "\n",
    "Once we get the attention mask, we combine it with the value matrix to get the final output from our attention layer. The easiest way to think of applying an attention mask is with an example from computer vision. The \"thing\" that we are trying to do with computer vision, say image recognition, is to capture information from the \"important part\" of the image. We don't want to focus, normally, on background stuff. The attention mask serves to act basically as a filter, that blocks out the less important and lets through the more important. So we can think of the end result as the input + mask = useful output. This image is a little blurry, but it shows the idea. If we have a model being trained to identify objects, we might end up with a mask that looks like this. Note the final result and the original (which has had the color space changed). The desired result is the bottom left, where the objects we want to identify are the focus. Applying the mask to the original serves to do that - remove the less important stuff, emphasize the more important stuff. With language, we get the same thing. We want to focus on the important parts of a sentence and ignore the less important parts - that measure of importance is what we are learning during training. \n",
    "\n",
    "![Attention Mask](images/attention_mask.png \"Attention Mask\")\n",
    "\n",
    "<b>Note:</b> we also have a causal mask, which is used to prevent the model from \"cheating\" by looking ahead in the input sequence. This effectively stops the model from just looking up the answer, which would let it sidestep learning. \n",
    "\n",
    "#### Multi-Head Attention\n",
    "\n",
    "The layer that we are adding is called a multi-head attention layer, implying that we have multiple attention filters at once. This part works similarly to how the convolutional filters work in a CNN. Each filter in a CNN learns to identify some useful feature in that context - edges, colors, etc... Here, each attention head learns to focus on a different aspect of the input, language in our case. As our model is trained, each attention head will learn to focus on different aspects of the input. Recall that the weights for the filter are normally random initially, so the training process will cause each one to find its own thing to focus on as we shrink the loss. \n",
    "\n",
    "### Attention Magic\n",
    "\n",
    "This is a very brief and high level overview of attention and its application to our neural networks. There is a lot more to it, it is a very interesting topic, and based on what we know now (2023), transformer based models will likely be exceedingly common over the near future. If you want to learn more, I recommend the following resources:\n",
    "<ul>\n",
    "<li> https://data-science-blog.com/blog/2021/04/07/multi-head-attention-mechanism/\n",
    "<li> https://www.youtube.com/watch?v=6D4EWKJgNn0\n",
    "<li> https://data-science-blog.com/blog/2021/04/22/positional-encoding-residual-connections-padding-masks-all-the-details-of-transformer-model/\n",
    "</ul>\n",
    "\n",
    "The ability of the transformer models to, without external direction, learn what is important and what is not is what makes them both so powerful and so flexible. The examples of the GPT models accurately performing tasks that it wasn't trained on are good examples of this flexibility. If we have training data to supply the transformer model, it can very accurately learn to extract what matters from what doesn't, irrespective of the specific task that it is working on, which makes learning that task much easier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def causal_attention_mask(batch_size, n_dest, n_src, dtype):\n",
    "    \"\"\"\n",
    "    Mask the upper half of the dot product matrix in self attention.\n",
    "    This prevents flow of information from future tokens to current token.\n",
    "    1's in the lower triangle, counting from the lower right corner.\n",
    "    \"\"\"\n",
    "    i = tf.range(n_dest)[:, None]\n",
    "    j = tf.range(n_src)\n",
    "    m = i >= j - n_src + n_dest\n",
    "    mask = tf.cast(m, dtype)\n",
    "    mask = tf.reshape(mask, [1, n_dest, n_src])\n",
    "    mult = tf.concat(\n",
    "        [tf.expand_dims(batch_size, -1), tf.constant([1, 1], dtype=tf.int32)], 0\n",
    "    )\n",
    "    return tf.tile(mask, mult)\n",
    "\n",
    "\n",
    "class TransformerBlock(layers.Layer):\n",
    "    def __init__(self, embed_dim, num_heads, ff_dim, rate=0.1):\n",
    "        super().__init__()\n",
    "        self.att = layers.MultiHeadAttention(num_heads, embed_dim)\n",
    "        self.ffn = keras.Sequential(\n",
    "            [layers.Dense(ff_dim, activation=\"relu\"), layers.Dense(embed_dim),]\n",
    "        )\n",
    "        self.layernorm1 = layers.LayerNormalization(epsilon=1e-6)\n",
    "        self.layernorm2 = layers.LayerNormalization(epsilon=1e-6)\n",
    "        self.dropout1 = layers.Dropout(rate)\n",
    "        self.dropout2 = layers.Dropout(rate)\n",
    "\n",
    "    def call(self, inputs):\n",
    "        input_shape = tf.shape(inputs)\n",
    "        batch_size = input_shape[0]\n",
    "        seq_len = input_shape[1]\n",
    "        causal_mask = causal_attention_mask(batch_size, seq_len, seq_len, tf.bool)\n",
    "        attention_output = self.att(inputs, inputs, attention_mask=causal_mask)\n",
    "        attention_output = self.dropout1(attention_output)\n",
    "        out1 = self.layernorm1(inputs + attention_output)\n",
    "        ffn_output = self.ffn(out1)\n",
    "        ffn_output = self.dropout2(ffn_output)\n",
    "        return self.layernorm2(out1 + ffn_output)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create Model\n",
    "\n",
    "We can now create the model, and we will use the portions that we constructed above. The basic parts are:\n",
    "<ul>\n",
    "<li> Token and positional embedding - create representations of each sequence. \n",
    "<li> Transformer layers - the core of the model.\n",
    "<li> Output layer - dense layers to convert the output of the transformer layers to the output of the model.\n",
    "</ul>\n",
    "\n",
    "The basic structure of different varieties of neural networks is also seen here, we again have a dense neural network to generate predictions from inputs, and that network can be fed by either:\n",
    "<ul>\n",
    "<li> Our actual data, for normal regression or classification.\n",
    "<li> The output of convolutional layers, for image processing. \n",
    "<li> The output of recurrent layers, for sequential data.\n",
    "<li> The output of transformer layers, for quickly expanding types of tasks. \n",
    "</ul>\n",
    "\n",
    "No matter the specific implementation, the basic structure, and ability to learn, is the same in all neural networks. The ability to learn relationships that are complex, obscure, and impossible for a human to describe makes neural networks extremely powerful. If we can generate some architecture that is good at extracting features from some specific type of data, we can combine that with a regular neural network to make all kinds of predictions or generate new data. Our \"predictor\" dense model, and the \"extractor\" early layers can then both learn epoch by epoch, together, to be as accurate as possible. As the capacity of processors increases and the experience of researchers grows, we can expect to see more and more expansion in what neural networks can do. In particular, the increased ability to parallelize the processing of sequential data with the transformer architecture is massively helpful - we saw in the LSTM models the depth of the sequences of calculations meant that growing models to be very powerful requires lots of processing, in a way that is extremely hard to parallelize, limiting the growth. Transformers can do more in parallel, and it is much easier to add another processor than it is to develop a processor that is twice as fast; these models will likely grow to more efficiently process data accross large networks of worker machines, generating larger and more powerful models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "HIGH_RESOURCE = False\n",
    "\n",
    "vocab_size = 10000  # Only consider the top X words\n",
    "maxlen = 60  # Max sequence size\n",
    "embed_dim = 196  # Embedding size for each token\n",
    "num_heads = 2  # Number of attention heads\n",
    "feed_forward_dim = 196  # Hidden layer size in feed forward network inside transformer\n",
    "\n",
    "batch_size = 512\n",
    "EPOCHS = 10\n",
    "\n",
    "if HIGH_RESOURCE:\n",
    "    vocab_size = 20000  # Only consider the top 20k words\n",
    "    maxlen = 80  # Max sequence size\n",
    "    embed_dim = 512  # Embedding size for each token\n",
    "    num_heads = 8  # Number of attention heads\n",
    "    feed_forward_dim = 512  # Hidden layer size in feed forward network inside transformer\n",
    "    batch_size = 1024\n",
    "    EPOCHS = 100\n",
    "\n",
    "def create_model():\n",
    "    inputs = layers.Input(shape=(maxlen,), dtype=tf.int32)\n",
    "    embedding_layer = TokenAndPositionEmbedding(maxlen, vocab_size, embed_dim)\n",
    "    x = embedding_layer(inputs)\n",
    "    transformer_block = TransformerBlock(embed_dim, num_heads, feed_forward_dim)\n",
    "    x = transformer_block(x)\n",
    "    outputs = layers.Dense(vocab_size)(x)\n",
    "    model = keras.Model(inputs=inputs, outputs=[outputs, x])\n",
    "    loss_fn = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True)\n",
    "    model.compile(\n",
    "        \"adam\", loss=[loss_fn, None],\n",
    "    )  # No loss and optimization based on word embeddings from transformer block\n",
    "    return model\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get Data and Prepare for Training\n",
    "\n",
    "This example uses some movie reviews for source data. The dataset comes already split into positive and negative labels, for classification, and into training and testing sets. We don't need any of these divisions, we just need all the text for training, so the data preparation steps here are:\n",
    "<ul>\n",
    "<li> Download the data.\n",
    "<li> Loop through all the files and generate a list of all the file names. \n",
    "<li> Crate a dataset from all the files. \n",
    "<li> Clean the data by removing the html tags and punctuation.\n",
    "<li> Tokenize the data by splitting the text into words and creating a vocabulary.\n",
    "<li> Create training ready data by creating sequences of X = \"up to the current word\" and Y = \"the next word\".\n",
    "<li> Set the dataset to be shuffled, batched, and prefetched.\n",
    "</ul>\n",
    "\n",
    "<b>Note:</b> there are a few odd [UNK] tokens, this is a placeholder for words that are not in the vocabulary. Were this a production model, we'd want to come up with some more sophisticated way of handling this, but for this example, we'll just leave it as is. When dealing with natural text, it is common to have things like this for unknown data, or other special tokens for the beginning or end of a sentence (e.g. [BOS] or [EOS]).  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\n",
      "                                 Dload  Upload   Total   Spent    Left  Speed\n",
      "100 80.2M  100 80.2M    0     0  3682k      0  0:00:22  0:00:22 --:--:-- 4804k0:34  0:00:10  0:00:24 3140k\n"
     ]
    }
   ],
   "source": [
    "!curl -O https://ai.stanford.edu/~amaas/data/sentiment/aclImdb_v1.tar.gz\n",
    "!tar -xf aclImdb_v1.tar.gz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "50000 files\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-04-19 11:40:18.439331: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    }
   ],
   "source": [
    "# The dataset contains each review in a separate text file\n",
    "# The text files are present in four different folders\n",
    "# Create a list all files\n",
    "filenames = []\n",
    "directories = [\n",
    "    \"aclImdb/train/pos\",\n",
    "    \"aclImdb/train/neg\",\n",
    "    \"aclImdb/test/pos\",\n",
    "    \"aclImdb/test/neg\",\n",
    "]\n",
    "for dir in directories:\n",
    "    for f in os.listdir(dir):\n",
    "        filenames.append(os.path.join(dir, f))\n",
    "\n",
    "print(f\"{len(filenames)} files\")\n",
    "\n",
    "# Create a dataset from text files\n",
    "random.shuffle(filenames)\n",
    "text_ds = tf.data.TextLineDataset(filenames)\n",
    "text_ds = text_ds.shuffle(buffer_size=256)\n",
    "text_ds = text_ds.batch(batch_size)\n",
    "\n",
    "\n",
    "def custom_standardization(input_string):\n",
    "    \"\"\" Remove html line-break tags and handle punctuation \"\"\"\n",
    "    lowercased = tf.strings.lower(input_string)\n",
    "    stripped_html = tf.strings.regex_replace(lowercased, \"<br />\", \" \")\n",
    "    return tf.strings.regex_replace(stripped_html, f\"([{string.punctuation}])\", r\" \\1\")\n",
    "\n",
    "\n",
    "# Create a vectorization layer and adapt it to the text\n",
    "vectorize_layer = TextVectorization(\n",
    "    standardize=custom_standardization,\n",
    "    max_tokens=vocab_size - 1,\n",
    "    output_mode=\"int\",\n",
    "    output_sequence_length=maxlen + 1,\n",
    ")\n",
    "vectorize_layer.adapt(text_ds)\n",
    "vocab = vectorize_layer.get_vocabulary()  # To get words back from token indices\n",
    "\n",
    "\n",
    "def prepare_lm_inputs_labels(text):\n",
    "    \"\"\"\n",
    "    Shift word sequences by 1 position so that the target for position (i) is\n",
    "    word at position (i+1). The model will use all words up till position (i)\n",
    "    to predict the next word.\n",
    "    \"\"\"\n",
    "    text = tf.expand_dims(text, -1)\n",
    "    tokenized_sentences = vectorize_layer(text)\n",
    "    x = tokenized_sentences[:, :-1]\n",
    "    y = tokenized_sentences[:, 1:]\n",
    "    return x, y\n",
    "\n",
    "\n",
    "text_ds = text_ds.map(prepare_lm_inputs_labels)\n",
    "text_ds = text_ds.prefetch(tf.data.AUTOTUNE)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can look at one example of the data below. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(256, 80) (256, 80)\n",
      "Tokens: [   12   218    13    22    37     2  3545 13500  2055    22  1384     3\n",
      "    13    22    74    84  1121    11     2     1  2028     8 16538    45\n",
      "    75    12   439    45    43    12    16   192   289   994     8    13\n",
      "    18     3    12    93   439  4251  1060     4 11159  7792   534     3\n",
      "     3     3    43    12    32    74    57  1163  2449    19  1084   956\n",
      "   329 13057   115     3   492     4    12   262    14 15583  3038   134\n",
      "    28  1643     1    19   109     7   145   108] \n",
      "\n",
      " [  218    13    22    37     2  3545 13500  2055    22  1384     3    13\n",
      "    22    74    84  1121    11     2     1  2028     8 16538    45    75\n",
      "    12   439    45    43    12    16   192   289   994     8    13    18\n",
      "     3    12    93   439  4251  1060     4 11159  7792   534     3     3\n",
      "     3    43    12    32    74    57  1163  2449    19  1084   956   329\n",
      " 13057   115     3   492     4    12   262    14 15583  3038   134    28\n",
      "  1643     1    19   109     7   145   108     3]\n",
      "Sentence: i saw this film at the 2005 edinburgh international film festival . this film had been compared in the [UNK] program to sideways - which i liked - so i was quite looking forward to this movie . i also liked garden state , napoleon dynamite etc . . . so i have had good recent experiences with slightly weird american indy films . unfortunately , i found that puffy chair does not compare [UNK] with any of these movies  \n",
      "\n",
      "Next word: .\n"
     ]
    }
   ],
   "source": [
    "tmp = text_ds.as_numpy_iterator()\n",
    "x_tmp, y_tmp = next(tmp)\n",
    "print(x_tmp.shape, y_tmp.shape)\n",
    "samp_x = x_tmp[0]\n",
    "samp_y = y_tmp[0]\n",
    "print(\"Tokens:\", samp_x, \"\\n\\n\", samp_y)\n",
    "word = \"\"\n",
    "for x_ in samp_x:\n",
    "    word += vocab[x_] + \" \"\n",
    "print(\"Sentence:\", word, \"\\n\\nNext word:\", vocab[samp_y[-1]])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Callback\n",
    "\n",
    "To get our text out, we can use a callback that will be called at the end of each epoch. We can still get things from \"predict\" after the fact, but this will give us some step by step evidence of our program's smarts. We will make two instances of this callback, each with different seeds. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TextGenerator(keras.callbacks.Callback):\n",
    "    \"\"\"A callback to generate text from a trained model.\n",
    "    1. Feed some starting prompt to the model\n",
    "    2. Predict probabilities for the next token\n",
    "    3. Sample the next token and add it to the next input\n",
    "\n",
    "    Arguments:\n",
    "        max_tokens: Integer, the number of tokens to be generated after prompt.\n",
    "        start_tokens: List of integers, the token indices for the starting prompt.\n",
    "        index_to_word: List of strings, obtained from the TextVectorization layer.\n",
    "        top_k: Integer, sample from the `top_k` token predictions.\n",
    "        print_every: Integer, print after this many epochs.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, max_tokens, start_tokens, index_to_word, top_k=10, print_every=1, log_dir=\"logs\"):\n",
    "        self.max_tokens = max_tokens\n",
    "        self.start_tokens = start_tokens\n",
    "        self.index_to_word = index_to_word\n",
    "        self.print_every = print_every\n",
    "        self.k = top_k\n",
    "        self.log_dir = log_dir\n",
    "        self.file_writer = tf.summary.create_file_writer(self.log_dir)\n",
    "\n",
    "    def sample_from(self, logits):\n",
    "        logits, indices = tf.math.top_k(logits, k=self.k, sorted=True)\n",
    "        indices = np.asarray(indices).astype(\"int32\")\n",
    "        preds = keras.activations.softmax(tf.expand_dims(logits, 0))[0]\n",
    "        preds = np.asarray(preds).astype(\"float32\")\n",
    "        return np.random.choice(indices, p=preds)\n",
    "\n",
    "    def detokenize(self, number):\n",
    "        return self.index_to_word[number]\n",
    "\n",
    "    def on_epoch_end(self, epoch):\n",
    "        start_tokens = [_ for _ in self.start_tokens]\n",
    "        if (epoch + 1) % self.print_every != 0:\n",
    "            return\n",
    "        num_tokens_generated = 0\n",
    "        tokens_generated = []\n",
    "        while num_tokens_generated <= self.max_tokens:\n",
    "            pad_len = maxlen - len(start_tokens)\n",
    "            sample_index = len(start_tokens) - 1\n",
    "            if pad_len < 0:\n",
    "                x = start_tokens[:maxlen]\n",
    "                sample_index = maxlen - 1\n",
    "            elif pad_len > 0:\n",
    "                x = start_tokens + [0] * pad_len\n",
    "            else:\n",
    "                x = start_tokens\n",
    "            x = np.array([x])\n",
    "            y, _ = self.model.predict(x)\n",
    "            sample_token = self.sample_from(y[0][sample_index])\n",
    "            tokens_generated.append(sample_token)\n",
    "            start_tokens.append(sample_token)\n",
    "            num_tokens_generated = len(tokens_generated)\n",
    "        txt = \" \".join(\n",
    "            [self.detokenize(_) for _ in self.start_tokens + tokens_generated]\n",
    "        )\n",
    "        print(f\"generated text:\\n{txt}\\n\")\n",
    "        with self.file_writer.as_default():\n",
    "            tf.summary.text(\"Text Data\", txt, step=epoch)\n",
    "        #self.file_writer.flush()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train, Run, Predict\n",
    "\n",
    "Now that the model is created, we can fit it to the training data then test out the abilities. Our prediction is an incremental process, we start with a seed, then we predict the next word, then we add that word to the seed, and predict the next word, and so on. At each step, the model looks at the input to this point, calculates the attention, finds the most suitable (highest score) word from the vocabulary, generates it, calculates the loss (N-dimensional \"closeness\" from embedding), and moves one more step forward. This training process can take a very long time, loss kept slowly improving for me for 100+ epochs without fully flattening. With larger models and datasets, this pattern will likely still occur, only at a far larger number of epochs. This is a very tangible example of the need for fast machines to turn around quick training cycles, look at the quality of the sample lines from epoch 1 to epoch 20+, the difference is often striking. Here is one example that I took from in the middle of training, at epoch 38:\n",
    "<ul>\n",
    "<li> <b>Epoch 1:</b> this movie is [UNK] to of . to of . , to the . a the a the , , . . the . the [UNK] the , the , [UNK] the of a the the . . and . is [UNK] to [UNK]\n",
    "<li> <b>Epoch 38:</b> this movie is not just a bad movie . the plot is outrageous and unbelievable . sure the characters are unbelievable and the ending will be a better surprise , but the ending was just sad .       \n",
    "</ul>\n",
    "Not a bad improvement. Like most models, we normally see a really quick improvement in the first few epochs, to get to \"decent\", we then usually get a really slow improvement as the model gets slightly better each epoch, for a long time. Here I am only looking at the loss, which isn't really a number that has contextual meaning on its own, but it gives us a metric of how well the model is doing in comparison to itself. The accuracy metric isn't really useful here as we want to generate \"a\" correct word, not specifically \"the\" correct word, so generating \"vehicle\" instead of \"automobile\" is still a good answer, as we'd expect them to be quite close in the N-dimensional embedding. There are other metrics that are commonly used to evaluate text generation, such as BLEU and Rouge, that aim to generate some form of an \"accuracy\" score for the text that is created for a model. Any accuracy measure when we are creating something for human consumption is only a guideline - our model can optimize for loss, but loss is only a proxy for \"the model thinks of a good next word\" - there isn't any way to directly calculate how close to \"real speach\" our generated data is. We won't get into those other metrics here, the loss and a subjective evaluation of the text is good enough for us.\n",
    "\n",
    "<b>Note:</b> Trying to train this on my laptop on CPU took forever, I didn't get to the point where the first epoch gave me a time estimate. On GPU it is much, much faster."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tokenize starting prompt\n",
    "word_to_index = {}\n",
    "for index, word in enumerate(vocab):\n",
    "    word_to_index[word] = index\n",
    "\n",
    "start_prompt1 = \"this movie is\"\n",
    "start_tokens1 = [word_to_index.get(_, 1) for _ in start_prompt1.split()]\n",
    "start_prompt2 = \"Skiing fast makes me\"\n",
    "start_tokens2 = [word_to_index.get(_, 1) for _ in start_prompt2.split()]\n",
    "num_tokens_generated = 40\n",
    "\n",
    "log_dir = \"logs\"\n",
    "log_1 = str(log_dir + \"/1\")\n",
    "log_2 = str(log_dir + \"/2\")\n",
    "text_gen_callback1 = TextGenerator(num_tokens_generated, start_tokens1, vocab, log_dir=log_1)\n",
    "text_gen_callback2 = TextGenerator(num_tokens_generated, start_tokens2, vocab, log_dir=log_2)\n",
    "tensorboard_callback = keras.callbacks.TensorBoard(log_dir=log_dir, histogram_freq=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Launch TensorBoard\n",
    "%load_ext tensorboard\n",
    "%tensorboard --logdir log_dir\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m/var/folders/px/vhm_920n7zx2wvqq_ht0q5tm0000gp/T/ipykernel_63994/2654523382.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcreate_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtext_ds\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mverbose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mEPOCHS\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcallbacks\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mtext_gen_callback1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtext_gen_callback2\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/opt/anaconda3/envs/ml3950/lib/python3.9/site-packages/keras/utils/traceback_utils.py\u001b[0m in \u001b[0;36merror_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     63\u001b[0m         \u001b[0mfiltered_tb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     64\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 65\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     66\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     67\u001b[0m             \u001b[0mfiltered_tb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_process_traceback_frames\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__traceback__\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/envs/ml3950/lib/python3.9/site-packages/keras/engine/training.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[1;32m   1648\u001b[0m                         ):\n\u001b[1;32m   1649\u001b[0m                             \u001b[0mcallbacks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mon_train_batch_begin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1650\u001b[0;31m                             \u001b[0mtmp_logs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0miterator\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1651\u001b[0m                             \u001b[0;32mif\u001b[0m \u001b[0mdata_handler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshould_sync\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1652\u001b[0m                                 \u001b[0mcontext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0masync_wait\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/envs/ml3950/lib/python3.9/site-packages/tensorflow/python/util/traceback_utils.py\u001b[0m in \u001b[0;36merror_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    148\u001b[0m     \u001b[0mfiltered_tb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    149\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 150\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    151\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    152\u001b[0m       \u001b[0mfiltered_tb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_process_traceback_frames\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__traceback__\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/envs/ml3950/lib/python3.9/site-packages/tensorflow/python/eager/polymorphic_function/polymorphic_function.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    878\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    879\u001b[0m       \u001b[0;32mwith\u001b[0m \u001b[0mOptionalXlaContext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jit_compile\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 880\u001b[0;31m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    881\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    882\u001b[0m       \u001b[0mnew_tracing_count\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexperimental_get_tracing_count\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/envs/ml3950/lib/python3.9/site-packages/tensorflow/python/eager/polymorphic_function/polymorphic_function.py\u001b[0m in \u001b[0;36m_call\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    910\u001b[0m       \u001b[0;31m# In this case we have created variables on the first call, so we run the\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    911\u001b[0m       \u001b[0;31m# defunned version which is guaranteed to never create variables.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 912\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_no_variable_creation_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# pylint: disable=not-callable\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    913\u001b[0m     \u001b[0;32melif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_variable_creation_fn\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    914\u001b[0m       \u001b[0;31m# Release the lock early so that multiple threads can perform the call\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/envs/ml3950/lib/python3.9/site-packages/tensorflow/python/eager/polymorphic_function/tracing_compiler.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    132\u001b[0m       (concrete_function,\n\u001b[1;32m    133\u001b[0m        filtered_flat_args) = self._maybe_define_function(args, kwargs)\n\u001b[0;32m--> 134\u001b[0;31m     return concrete_function._call_flat(\n\u001b[0m\u001b[1;32m    135\u001b[0m         filtered_flat_args, captured_inputs=concrete_function.captured_inputs)  # pylint: disable=protected-access\n\u001b[1;32m    136\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/envs/ml3950/lib/python3.9/site-packages/tensorflow/python/eager/polymorphic_function/monomorphic_function.py\u001b[0m in \u001b[0;36m_call_flat\u001b[0;34m(self, args, captured_inputs, cancellation_manager)\u001b[0m\n\u001b[1;32m   1743\u001b[0m         and executing_eagerly):\n\u001b[1;32m   1744\u001b[0m       \u001b[0;31m# No tape is watching; skip to running the function.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1745\u001b[0;31m       return self._build_call_outputs(self._inference_function.call(\n\u001b[0m\u001b[1;32m   1746\u001b[0m           ctx, args, cancellation_manager=cancellation_manager))\n\u001b[1;32m   1747\u001b[0m     forward_backward = self._select_forward_and_backward_functions(\n",
      "\u001b[0;32m~/opt/anaconda3/envs/ml3950/lib/python3.9/site-packages/tensorflow/python/eager/polymorphic_function/monomorphic_function.py\u001b[0m in \u001b[0;36mcall\u001b[0;34m(self, ctx, args, cancellation_manager)\u001b[0m\n\u001b[1;32m    376\u001b[0m       \u001b[0;32mwith\u001b[0m \u001b[0m_InterpolateFunctionError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    377\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mcancellation_manager\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 378\u001b[0;31m           outputs = execute.execute(\n\u001b[0m\u001b[1;32m    379\u001b[0m               \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msignature\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    380\u001b[0m               \u001b[0mnum_outputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_num_outputs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/envs/ml3950/lib/python3.9/site-packages/tensorflow/python/eager/execute.py\u001b[0m in \u001b[0;36mquick_execute\u001b[0;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[1;32m     50\u001b[0m   \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     51\u001b[0m     \u001b[0mctx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mensure_initialized\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 52\u001b[0;31m     tensors = pywrap_tfe.TFE_Py_Execute(ctx._handle, device_name, op_name,\n\u001b[0m\u001b[1;32m     53\u001b[0m                                         inputs, attrs, num_outputs)\n\u001b[1;32m     54\u001b[0m   \u001b[0;32mexcept\u001b[0m \u001b[0mcore\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_NotOkStatusException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "model = create_model()\n",
    "model.fit(text_ds, verbose=1, epochs=EPOCHS, callbacks=[text_gen_callback1, text_gen_callback2, tensorboard_callback])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Predictions\n",
    "\n",
    "We can now create some predicted text with our trained model. Below are just some helper functions to make predictions of a certain length. The quality of the text is highly, highly variable - the dataset used was only one source, and wasn't massive. The model is also pretty small. I ran this once for 100 epochs of training, and the results I got for generated text were:\n",
    "<ul>\n",
    "<li> this movie is not really a bad movie . there are only two stars (the heroine ) almost all vampire tales being [UNK] i psycho if your life . it wasn 't that part of this movie . i wanted to see more like that , but i just have to say i really liked it that movie and even the music was . in my opinion the movie itself and it really gets a look like something that you might can just really can just a bad movie . there are only two stars (the heroine ) almost all vampire tales being [UNK] i psycho if your life . it wasn 't that part of this movie . i wanted to see more like that , but i just have to say i really liked it that movie and even the music was . in my opinion the movie itself and it really gets a look like something that you might can just really can just \n",
    "<li> fast makes me forget . check . when you think carl brashear [UNK] gooding jr . and his navy master chief diver bill finds himself inside his hands , and even by his face running his navy , deep and surface story of carl brashear 's brother , played by cuba gooding jr . he died in his obsession with his cat competition . . the physical comedy gives him great example of his first powerful karate and kung fu and and master . forget . check . when you think carl brashear [UNK] gooding jr . and his navy master chief diver bill finds himself inside his hands , and even by his face running his navy , deep and surface story of carl brashear 's brother , played by cuba gooding jr . he died in his obsession with his cat competition . . the physical comedy gives him great example of his first powerful karate and kung fu and and master . \n",
    "<li> are going to make this country great movie on which you think the best film ever ? i am from a big city photographer . but then i saw \"one dark night [UNK] summer \" , and i left the store , the week later when i hit rock channels . i see 15 [UNK] only two films that have noted a very strange mixture of suspense and interaction between nature and noble and i found the kind of movies dimension . . . . . . . movie on which you think the best film ever ? i am from a big city photographer . but then i saw \"one dark night [UNK] summer \" , and i left the store , the week later when i hit rock channels . i see 15 [UNK] only two films that have noted a very strange mixture of suspense and interaction between nature and noble and i found the kind of movies dimension . . . . . . . \n",
    "<li> my dogs at that point i can 't say i am impressed with this movie for the first 10 years but i don 't remember any things i liked were in this animated movie , but there were something good in it . the animation is narration from the music , the songs were played : \"you sons ' \" and family how a kid died until after 9 years ! \" . it was not that bad it was done bad done used that point i can 't say i am impressed with this movie for the first 10 years but i don 't remember any things i liked were in this animated movie , but there were something good in it . the animation is narration from the music , the songs were played : \"you sons ' \" and family how a kid died until after 9 years ! \" . it was not that bad it was done bad done used\n",
    "</ul>\n",
    "\n",
    "Not amazing, but not terrible either. We have results that are more or less sentences, with some parts where we really go off the rails. For a small model, short training, and tiny dataset, I'd say we are doing reasonably well. Below, the sample_k value is somewhat similar to our Temperature parameter. This controls the amount of randomness in the prediction by controling the number of values that we are choosing from when generating a word. This k represents the number of most-likely words we are choosing from, so if it is 5, our generated word will come from the top 5 most likely words. If it is 1, we will only choose from the most likely word. This is called top_k sampling, it is a pretty simple method to add randomness to a generative model. As with any model, as we move farther out, we get more and more weird results - that really requires a larger model to capture well. There are several different ways that this selection process can be done, with each method applying a different algorithm to add randomness to the final generated token, this link is a good overview of some of the different choices at a high level: https://towardsdatascience.com/decoding-strategies-that-you-need-to-know-for-response-generation-ba95ee0faadc "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def indToSentence(ind, dict):\n",
    "    word = \"\"\n",
    "    for n_ in ind:\n",
    "        word += dict[n_] + \" \"\n",
    "    return word\n",
    "\n",
    "def sentenceToInd(sentence, dict):\n",
    "    indicies = []\n",
    "    words = sentence.split()\n",
    "    for word in words:\n",
    "        if isinstance(word, str):\n",
    "            index = dict.get(word)\n",
    "            if index is not None:\n",
    "                indicies.append(index)\n",
    "    return indicies\n",
    "\n",
    "def sample_from(logits, sample_k = 2):\n",
    "    logits, indices = tf.math.top_k(logits, k=sample_k, sorted=True)\n",
    "    indices = np.asarray(indices).astype(\"int32\")\n",
    "    preds = keras.activations.softmax(tf.expand_dims(logits, 0))[0]\n",
    "    preds = np.asarray(preds).astype(\"float32\")\n",
    "    return np.random.choice(indices, p=preds)\n",
    "\n",
    "def generateText(model, index_to_word, word_to_index, startPrompt, length=80, sample_k = 12):\n",
    "    start_tokens = sentenceToInd(startPrompt, word_to_index)\n",
    "    num_tokens_generated = 0\n",
    "    tokens_generated = []\n",
    "    while num_tokens_generated <= length:\n",
    "        pad_len = maxlen - len(start_tokens)\n",
    "        sample_index = len(start_tokens) - 1\n",
    "        if pad_len < 0:\n",
    "            x = start_tokens[:maxlen]\n",
    "            sample_index = maxlen - 1\n",
    "        elif pad_len > 0:\n",
    "            x = start_tokens + [0] * pad_len\n",
    "        else:\n",
    "            x = start_tokens\n",
    "        x = np.array([x])\n",
    "        y, _ = model.predict(x)\n",
    "        #sample_token = np.argmax(y[0][sample_index])\n",
    "        logits = y[0][sample_index]\n",
    "        sample_token = sample_from(logits, sample_k)\n",
    "        tokens_generated.append(sample_token)\n",
    "        start_tokens.append(sample_token)\n",
    "        num_tokens_generated = len(tokens_generated)\n",
    "    txt = indToSentence(start_tokens + tokens_generated, index_to_word)\n",
    "    return txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "k_size = 15\n",
    "\n",
    "t1 = generateText(model, vocab, word_to_index, \"this movie is not really\", sample_k = k_size)\n",
    "t2 = generateText(model, vocab, word_to_index, \"Skiing fast makes me\", sample_k = k_size)\n",
    "t3 = generateText(model, vocab, word_to_index, \"We are going to make this country great\", sample_k = k_size)\n",
    "t4 = generateText(model, vocab, word_to_index, \"Where my dogs at\", sample_k = k_size)\n",
    "t5 = generateText(model, vocab, word_to_index, \"How many licks would it take to\", sample_k = k_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(t1)\n",
    "print(t2)\n",
    "print(t3)\n",
    "print(t4)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Transformer Wrap-Up and Use\n",
    "\n",
    "Transformers are new, so we don't have all that much experience using them and applying them to a variety of problems. To this point, it looks very likely that transformer based models will be the leaders in sequential data, and potentially much more. The key ability of the transformer to generate \"attention\" between any two values in a sequence, in parallel, is a massive advantage over LSTM models. This advantage lies both in the logic of the model as well as the ability to more efficiently parallelize the processing, which is hard with seqential data. The ability of transformer models to adapt to many different types of problems, with little to no explicit direction, opens the door to many innovations in the near future. Some of the most exciting work is being done in the area of \"few shot learning\", where we can take a model that has been trained on a large amount of data, and then use that model to learn a new task with very little data. As transformer models progress, we'll likely see them edging out other architectures such as RNNs in many applications. \n",
    "\n",
    "The current massively impressive application of transformers is in larger versions of what we are doing, large language models. These models are, at their core, very similar to what we have created, the major difference being:\n",
    "<ul>\n",
    "<li> Far more training text allows the model to use the ability to learn from context to generate a far more accurate understanding of the structure of language. \n",
    "<li> A far larger model allows the model to \"hold\" a large vocabulary and a large number of relationships between words.\n",
    "<li> Many large models utilize some form of human feedback to improve the model, generally by having a human vote for the best sentence generated by the model, or give a thumbs up/down to what a model produces. This adds in some human supervised learning, which can be important in helping the model become more natural sounding. \n",
    "</ul>\n",
    "\n",
    "At the time of writing, the current state-of-the-art in language models is GPT4, which is a much larger evolution from the globally impactful ChatGPT. As computers get better, we should see models that are larger and smarter, especially as the transformer architecture allows us to scalue up models by parallelizing the processing. Costs just to train some of these models are counted in the millions of dollars, either from renting cloud resources or purchasing GPUs and paying for electricity, so this is one of relatively few computing problems where computing speed and efficiency are critical limitations. Researchers are working on an assortment of ways to make these training times more efficient, from reducing the precision of calculations inside the neural networks (saving time for each +-*/ operation), to creating more efficent optimization algorithms for gradient descent, to building hardware that is inherently more efficient at training these models. One of the unique things about predictive modelling is that, generally, faster hardware not only means models are trained faster, it means that models can be trained <i>better</i>. Even without any improvements to the code, a model that can try more hyperparameter combinations in a grid search, process more training data, or train for more epochs, will be able to find a better solution. In this sense, faster GPUs and smarter algorithms are two sides of the same performance/accuracy coin; more speed = more attempts = more training = better model.\n",
    "\n",
    "Due to these factors, as well as the increasing number of people with knowledge of neural networks and machine learning, we can expect to see the rate of progress in this area to continue to accelerate, potentially to a shocking extent. Simply adding more data, using newer hardware, and refining existing code will naturally lead to all of these large models getting better and better as time progresses. With a large number of very smart people working on the implementation of the transformers in code, we can also expect to see both general refinements as well as the potential for some big jumps in ability as we've hinted at. Large language models have really only been a thing that the general public has known about since ChatGPT, so the increased exposure should lead to more brains thinking about it, and more innovation. Since this specific type of model has only existed for ~5 years, there's probably a lot of runway left for incremental improvements - I recently saw a paper on reducing or compressing the number of associations that are saved in the model, as many words in a sentence may not have \"attention\" needed between them; reducing that means fewer weights, less storage, less RAM, and faster processing - assuming you can keep the data you need to make the model! There are also improvements in a model's ability to train itself or generate training data. We looked at augmenting image data, a relatively simple way to add training data to an image model. The ability to create adversarial networks, which generate data then evaluate it as fake or real, also has the potential to speed the development of large neural networks. If we can create feedback that is \"good enough\" for training, we can let models just train indefinately and keep improving; we commonly see this type of approach on models that play games like chess, they play against themselves until they are amazing. A relatively small improvement in any of the underlying constraints, such as the ability to perform the weight update calculations slightly faster, will probably lead to outsized improvements in model performance down the line. \n",
    "\n",
    "This quick and accellerating development of the abilities of predictive models is extremely exciting, but also somewhat worrying. There is generally a low understanding of AI tools in the population, and the felt impact of these tools will be large. I'd predict that developments in neural networks feeds several unpredicatable changes in society:\n",
    "<ul>\n",
    "<li> Discharging of responsibility - we'll likely see more use of models making critical decisions with little to no human interaction. Take a self-driving car as an example, if the model that controls the car decides to drive it into a bunch of 5 year olds, who is responsible? The driver is currently, but that doesn't seem correct. I doubt your car insurance is excited to pay for Tesla's mistakes.\n",
    "<li> Automation of bias - as an extension to the point above, models can make decisions that are biased, and implicitly excuse that bias in the process. We'll probably see this in things like hiring decisions and loan approvals, where the people making or using the model may not explicitly intend to be biased, but the old training data is. We can see this in the film Coded Bias, where facial recognition models work far better on white men, as that is the demographic that was used to train the model.\n",
    "<li> Automation of jobs - as models become more and more capable, we'll see more and more jobs that are automated. This is already happening, but it will accelerate as models become more and more capable. This will lead to a lot of people being displaced from their jobs, often unpredicably and en masse. Once a self-driving semi-truck is \"good enough\" to replace a human driver, anyone using a human driver is at a massive disadvantage in the market. Potentially more concerning is knowledge and creative work - once AI is a little bit better, it'll be able to be tasked to do things like generate articles on today's news and format them into a newspaper, or generate illustrations to pair with the text of a children's book. This ability to be able to create content for essentially $0 may generate massive shifts in the very concept of creation. \n",
    "<li> Ownership of creation - as an extension of above, artists create art and they hold the license to that art. If a model can look at all art that has already been created and generate new art from that, who owns what is generated? Large players like Google, Amazon, and Apple have the ability to process much more data than any individual; can these companies simply scan every museum, art gallery, song, book, cartoon, anime, and movie in existence, generate new content, copyright that content, and basically choke out the ability of any individual to create anything that succeeds in the market? Disney has scores of original characters (including Marvel and Star Wars) along with a massive library of content and a ruthlessly agressive legal division. There is a realistic scenario in the near future where Disney can simply automate the production of new content, requring humans only to do a little polishing on the final product - no actors, voice actors, writers, animators, etc. needed. With enough processing time and data, a model could even generate new content dynamically, based on what people are looking for, watching, or seeing in the real world, then copyright this content for Disney - content could even be created that is tailored to the interests of one specific viewer. New content coming from humans would be a novelty, and it would be very difficult to compete with movies that can be generated by a single prompt and a few hours of processing time.\n",
    "<li> Fake news - finally, as an extension of the last point, generative AI has the ability to seriously change the idea of truth. Right now, a video of someone doing something is generally assumed to be true. We can edit or create fake videos, but it takes time and effort, so it is unlikely that someone will create a high quality but fake video outside of something like a movie. As tools get better and processors get cheaper, this balance changes. We will probably see deepfake videos offered as evidence in court, used to sway elections, or to craft fully fake naratives. Based on recent events, conservative parties accross the globe will likely spawn a cottage industry of realistic looking, but totally fake, videos of their opponents doing all kinds of embarrassing things.\n",
    "</ul>"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ml3950",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "4d722d3adfa415172c1f5238b519fb86b488acdae450fd691ab06c09f4ca9173"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
