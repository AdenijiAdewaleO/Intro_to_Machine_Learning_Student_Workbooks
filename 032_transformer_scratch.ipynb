{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-04-19 11:31:32.900850: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "from tensorflow.keras.layers import TextVectorization\n",
    "import numpy as np\n",
    "import os\n",
    "import re\n",
    "import string\n",
    "import random\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# WhackGPT\n",
    "\n",
    "We can make a transformer based model to generate chatGPT-ish text responses. Ours will be far more stupid, but hey, it's a taking computer. \n",
    "\n",
    "## Transformer Architecture\n",
    "\n",
    "What the heck is a transformer, what does it do, and why is it so cool? A transformer model is a type of neural network that was creating in 2017 at Google. The core idea behind transformers is the idea of attention, which is deailed a little bit below. The diagramed strucutre of a transformer model can be a little intimidating, but we can make sense of the critical parts without too much issue. \n",
    "\n",
    "![Transformer](images/transformer.png \"Transformer\")\n",
    "\n",
    "A transformer model contains a few key parts, each of with is dealt with in more detail below.\n",
    "<ul>\n",
    "<li> Embedding - the embedding layer generates embeddings (vector representations) for each token. The embeddings are created for both the token itself and its position in the sequence. </li>\n",
    "<li> Attention layers - the attention layers are the core of the transformer model. They are responsible for creating a representation of the input sequence that is used to generate the output sequence. </li>\n",
    "<li> Encoder - the encoder is a stack of attention layers that are used to create a representation of the input sequence. </li>\n",
    "<li> Decoder - the decoder is a stack of attention layers that are used to create a representation of the output sequence. </li>\n",
    "</ul>\n",
    "\n",
    "The attention part is the star of the show, it is a method to be able to focus the attention of the model on the critical portions of the input sequence and generate contextually informed predictions for the output. As well, transformers do all of this in a way that is more parallelizable than LSTM based models that were the state of the art before transformers, only a few years ago. "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Embedding\n",
    "\n",
    "The embedding here has two parts:\n",
    "<ul>\n",
    "<li> Token embedding: This maps each token to a vector representation in N-dimensional space. This is what we are used to for embedding. The original transformer paper used a 512-dimensional embedding, so each token was represented by a vector of 512 values that position it on a 512D grid. \n",
    "<li> Positional embedding: This maps each token's position in the <i>sequence</i>. The position embedding can be thought of as an extension of the concept of just tracking which word of a sentence each token is, 1,2,3...\n",
    "</ul>\n",
    "\n",
    "#### Token Embedding\n",
    "\n",
    "Token embedding is something that we are used to from when we used word2vec to generate embeddings for classification models. We are tranlating each token into an N-dimensional representation in space. The big difference here is that our embedding space is being learned by the model during training, so we should expect that the model will be shifting each token around in space as it learns more about what that word means, or more accurately, how it is used in our training data. \n",
    "\n",
    "![Embedding](images/embedding.png \"Embedding\")\n",
    "\n",
    "#### Positional Embedding\n",
    "\n",
    "The positional embedding is needed and most clearly seen if we compare this to an LSTM. In an LSTM, the position of a token is always known as we process the data sequentially. In the transformer model, the data is taken in parallel, so we don't have the sequence data built in. This has the benefit of allowing the model to process more of its work in parallel than an LSTM, but it also means that the model needs to be told where each token is in the sequence. What is the positional embedding? It follows the same concept as the token embedding, we are representing something with a vector of values. In the positional embedding, the math is a little involved, but it uses sine and cosine functions to represent the position of a token. \n",
    "\n",
    "![Positional Embedding](images/positional_emb.png \"Positional Embedding\")\n",
    "\n",
    "Where:\n",
    "<ul>\n",
    "<li> <b>k:</b> position of the token. \n",
    "<li> <b>d:</b> dimension of the embedding.\n",
    "<li> <b>i:</b> used for mapping to both sine and cosine functions.\n",
    "</ul>\n",
    "\n",
    "This positional embedding uses the trig functions to introduce some additional capability to our embedding values. First, this helps if we encounter longer sentences later on - if we embedded the position with a simple word count number, that would be an issue for us. Second, the trig functions allow us to embed the position in a way that is not deterministic. This means that the model can learn where tokens occur in relation to each other without being told explicitly. This is useful if you think of sentences such as:\n",
    "<ul>\n",
    "<li> I do not like the story of the movie, but I do like the cast.\n",
    "<li> I do like the story of the movie, but I do not like the cast.\n",
    "</ul>\n",
    "\n",
    "These two sentences use the same words, but the meaning is opposite. The positional embedding helps capture the relationship between the words based on where the occur, and connect words that occur in certain \"areas\" to those in other \"areas\" of a sentence. This is really useful if you think of something like an adjective, that adjective modifies some noun, and understanding English requires that we are able to identify which noun it belongs to. Positional embedding with sine/cosine help with that, the position is recorded not only in a way that tells us where a word sits in an absolute sense, but it tells us where that word sits relative to the other words it is with. This is one reason transformers are so useful for tasks like language, their ability to contextualize the relationships in parts of text surpasses that of other models that we have today; when generating text, this gives us the most natural sounding text, as the \"next word\" prediction is based on a more comprehensive understanding of the sentence. \n",
    "\n",
    "Notably, the positional embedding uses the word embedding dimension, d, as the dimension of the positional embedding. This is because the positional embedding is added to the token embedding, so the two need to be the same dimension. This means that the embedding matrix generated can be quite large for each token. This also means that the input to any future modelling is going to contain those two vectors, likely represented in a high dimension - what is the token, and where is it in the sequence."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TokenAndPositionEmbedding(layers.Layer):\n",
    "    def __init__(self, maxlen, vocab_size, embed_dim):\n",
    "        super().__init__()\n",
    "        self.token_emb = layers.Embedding(input_dim=vocab_size, output_dim=embed_dim)\n",
    "        self.pos_emb = layers.Embedding(input_dim=maxlen, output_dim=embed_dim)\n",
    "\n",
    "    def call(self, x):\n",
    "        maxlen = tf.shape(x)[-1]\n",
    "        positions = tf.range(start=0, limit=maxlen, delta=1)\n",
    "        positions = self.pos_emb(positions)\n",
    "        x = self.token_emb(x)\n",
    "        return x + positions"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Transformer Construction\n",
    "\n",
    "We can now create a function to construct the core piece of our model, the transformer. The transformer layer has a few parts, the critical one being the attention layer. \n",
    "\n",
    "<b>Note:</b> the declarations of the layers are slightly different in a functional model. Each layer is a function that takes an input tensor and returns an output tensor. The layers are then called in the call method of the model.\n",
    "\n",
    "### Pay (Multi-Head) Attention\n",
    "\n",
    "The core piece of the transformer architecture is the attention mechanism. Attention serves as a way to focus, or pay attention, to certain parts of the input.\n",
    "\n",
    "The attention mechanism contains three key matrices that we'll ultimately use to calculate things:\n",
    "<ul>\n",
    "<li> Query\n",
    "<li> Key\n",
    "<li> Value\n",
    "</ul>\n",
    "\n",
    "The query, key, values are commonly described as analagous to doing a Google search. For example, when you search for videos on Youtube, the search engine will map your <b>query</b> (text in the search bar) against a set of <b>keys</b> (video title, description, etc.) associated with candidate videos in their database, then present you the best matched <b>values</b> (videos).  \n",
    "\n",
    "Using the query, key, and value objects involves a multistep process. \n",
    "<ul>\n",
    "<li> First, the query, key, and value all get a copy of the embedding (position and token) matrix fed in, which is then multipled by a set of weights that belong to a linear layer (no activation) for that Q/K/V input. \n",
    "<li> The value matrix is set aside for the moment. \n",
    "<li> The results of the query and key matricies are then mutipled by each other, which generates attention scores. \n",
    "<li> The result is then passed through a softmax function to normalize the weights and generates the actual attention mask. \n",
    "<li> The normalized weights are then multiplied by the value matrix, which gives us the final output.\n",
    "</ul>\n",
    "\n",
    "To ultimately create the layer, we have several of these heads, similar to filters in a CNN. \n",
    "\n",
    "![Multi-Head Attention](images/multi_head_att.png \"Multi-Head Attention\")\n",
    "\n",
    "Take and example of a sentence being, \"“Anthony Hopkins admired Michael Bay as a great director\", the product of the query and key matricies would look something like this:\n",
    "\n",
    "![Attention Mechanism](images/key_value.png \"Attention Mechanism\")\n",
    "\n",
    "These attention scores are measures of how important each word in the input sequence is to each other word. We normally see each word being really important to itself, then as the similarity decreases, the importance decreases. In this example, \"Hopkins\" and \"Anthony\" have a high score of attention with respect to each other, which makes sense! We would likely want to produce those two words in sequence. Given large amounts of data, the model can become very good at identifying what is important and what is not, and in particular, understanding context. Because the attention is based on the positon and token embeddings, and we have multiple heads (see below) each honing in on some other aspect of the text, the model can learn relationships between parts of speech that are challenging for other types of models, such as a sentence that has a lage independent clause in the middle of it or figures of speach that have little impact on the meaning of a sentence. Importantly, each token in a sentence is taken as the input, so we generate such a matrix for each \"query\" token.\n",
    "\n",
    "![Attention Sequences](images/attention_seq.png \"Attention Sequences\")\n",
    "\n",
    "#### Attention Masking\n",
    "\n",
    "Once we get the attention mask, we combine it with the value matrix to get the final output from our attention layer. The easiest way to think of applying an attention mask is with an example from computer vision. The \"thing\" that we are trying to do with computer vision, say image recognition, is to capture information from the \"important part\" of the image. We don't want to focus, normally, on background stuff. The attention mask serves to act basically as a filter, that blocks out the less important and lets through the more important. So we can think of the end result as the input + mask = useful output. This image is a little blurry, but it shows the idea. If we have a model being trained to identify objects, we might end up with a mask that looks like this. Note the final result and the original (which has had the color space changed). The desired result is the bottom left, where the objects we want to identify are the focus. Applying the mask to the original serves to do that - remove the less important stuff, emphasize the more important stuff. With language, we get the same thing. We want to focus on the important parts of a sentence and ignore the less important parts - that measure of importance is what we are learning during training. \n",
    "\n",
    "![Attention Mask](images/attention_mask.png \"Attention Mask\")\n",
    "\n",
    "<b>Note:</b> we also have a causal mask, which is used to prevent the model from \"cheating\" by looking ahead in the input sequence. This effectively stops the model from just looking up the answer, which would let it sidestep learning. \n",
    "\n",
    "#### Multi-Head Attention\n",
    "\n",
    "The layer that we are adding is called a multi-head attention layer, implying that we have multiple attention filters at once. This part works similarly to how the convolutional filters work in a CNN. Each filter in a CNN learns to identify some useful feature in that context - edges, colors, etc... Here, each attention head learns to focus on a different aspect of the input, language in our case. As our model is trained, each attention head will learn to focus on different aspects of the input. Recall that the weights for the filter are normally random initially, so the training process will cause each one to find its own thing to focus on as we shrink the loss. \n",
    "\n",
    "### Attention Magic\n",
    "\n",
    "This is a very brief and high level overview of attention and its application to our neural networks. There is a lot more to it, it is a very interesting topic, and based on what we know now (2023), transformer based models will likely be exceedingly common over the near future. If you want to learn more, I recommend the following resources:\n",
    "<ul>\n",
    "<li> https://data-science-blog.com/blog/2021/04/07/multi-head-attention-mechanism/\n",
    "<li> https://www.youtube.com/watch?v=6D4EWKJgNn0\n",
    "<li> https://data-science-blog.com/blog/2021/04/22/positional-encoding-residual-connections-padding-masks-all-the-details-of-transformer-model/\n",
    "</ul>\n",
    "\n",
    "The ability of the transformer models to, without external direction, learn what is important and what is not is what makes them both so powerful and so flexible. The examples of the GPT models accurately performing tasks that it wasn't trained on are good examples of this flexibility. If we have training data to supply the transformer model, it can very accurately learn to extract what matters from what doesn't, irrespective of the specific task that it is working on, which makes learning that task much easier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def causal_attention_mask(batch_size, n_dest, n_src, dtype):\n",
    "    \"\"\"\n",
    "    Mask the upper half of the dot product matrix in self attention.\n",
    "    This prevents flow of information from future tokens to current token.\n",
    "    1's in the lower triangle, counting from the lower right corner.\n",
    "    \"\"\"\n",
    "    i = tf.range(n_dest)[:, None]\n",
    "    j = tf.range(n_src)\n",
    "    m = i >= j - n_src + n_dest\n",
    "    mask = tf.cast(m, dtype)\n",
    "    mask = tf.reshape(mask, [1, n_dest, n_src])\n",
    "    mult = tf.concat(\n",
    "        [tf.expand_dims(batch_size, -1), tf.constant([1, 1], dtype=tf.int32)], 0\n",
    "    )\n",
    "    return tf.tile(mask, mult)\n",
    "\n",
    "\n",
    "class TransformerBlock(layers.Layer):\n",
    "    def __init__(self, embed_dim, num_heads, ff_dim, rate=0.1):\n",
    "        super().__init__()\n",
    "        self.att = layers.MultiHeadAttention(num_heads, embed_dim)\n",
    "        self.ffn = keras.Sequential(\n",
    "            [layers.Dense(ff_dim, activation=\"relu\"), layers.Dense(embed_dim),]\n",
    "        )\n",
    "        self.layernorm1 = layers.LayerNormalization(epsilon=1e-6)\n",
    "        self.layernorm2 = layers.LayerNormalization(epsilon=1e-6)\n",
    "        self.dropout1 = layers.Dropout(rate)\n",
    "        self.dropout2 = layers.Dropout(rate)\n",
    "\n",
    "    def call(self, inputs):\n",
    "        input_shape = tf.shape(inputs)\n",
    "        batch_size = input_shape[0]\n",
    "        seq_len = input_shape[1]\n",
    "        causal_mask = causal_attention_mask(batch_size, seq_len, seq_len, tf.bool)\n",
    "        attention_output = self.att(inputs, inputs, attention_mask=causal_mask)\n",
    "        attention_output = self.dropout1(attention_output)\n",
    "        out1 = self.layernorm1(inputs + attention_output)\n",
    "        ffn_output = self.ffn(out1)\n",
    "        ffn_output = self.dropout2(ffn_output)\n",
    "        return self.layernorm2(out1 + ffn_output)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create Model\n",
    "\n",
    "We can now create the model, and we will use the portions that we constructed above. The basic parts are:\n",
    "<ul>\n",
    "<li> Token and positional embedding - create representations of each sequence. \n",
    "<li> Transformer layers - the core of the model.\n",
    "<li> Output layer - dense layers to convert the output of the transformer layers to the output of the model.\n",
    "</ul>\n",
    "\n",
    "The basic structure of different varieties of neural networks is also seen here, we again have a dense neural network to generate predictions from inputs, and that network can be fed by either:\n",
    "<ul>\n",
    "<li> Our actual data, for normal regression or classification.\n",
    "<li> The output of convolutional layers, for image processing. \n",
    "<li> The output of recurrent layers, for sequential data.\n",
    "<li> The output of transformer layers, for quickly expanding types of tasks. \n",
    "</ul>\n",
    "\n",
    "No matter the specific implementation, the basic structure, and ability to learn, is the same in all neural networks. The ability to learn relationships that are complex, obscure, and impossible for a human to describe makes neural networks extremely powerful. If we can generate some architecture that is good at extracting features from some specific type of data, we can combine that with a regular neural network to make all kinds of predictions or generate new data. Our \"predictor\" dense model, and the \"extractor\" early layers can then both learn epoch by epoch, together, to be as accurate as possible. As the capacity of processors increases and the experience of researchers grows, we can expect to see more and more expansion in what neural networks can do. In particular, the increased ability to parallelize the processing of sequential data with the transformer architecture is massively helpful - we saw in the LSTM models the depth of the sequences of calculations meant that growing models to be very powerful requires lots of processing, in a way that is extremely hard to parallelize, limiting the growth. Transformers can do more in parallel, and it is much easier to add another processor than it is to develop a processor that is twice as fast; these models will likely grow to more efficiently process data accross large networks of worker machines, generating larger and more powerful models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab_size = 20000  # Only consider the top 20k words\n",
    "maxlen = 80  # Max sequence size\n",
    "embed_dim = 256  # Embedding size for each token\n",
    "num_heads = 2  # Number of attention heads\n",
    "feed_forward_dim = 256  # Hidden layer size in feed forward network inside transformer\n",
    "\n",
    "\n",
    "def create_model():\n",
    "    inputs = layers.Input(shape=(maxlen,), dtype=tf.int32)\n",
    "    embedding_layer = TokenAndPositionEmbedding(maxlen, vocab_size, embed_dim)\n",
    "    x = embedding_layer(inputs)\n",
    "    transformer_block = TransformerBlock(embed_dim, num_heads, feed_forward_dim)\n",
    "    x = transformer_block(x)\n",
    "    outputs = layers.Dense(vocab_size)(x)\n",
    "    model = keras.Model(inputs=inputs, outputs=[outputs, x])\n",
    "    loss_fn = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True)\n",
    "    model.compile(\n",
    "        \"adam\", loss=[loss_fn, None],\n",
    "    )  # No loss and optimization based on word embeddings from transformer block\n",
    "    return model\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get Data and Prepare for Training\n",
    "\n",
    "This example uses some movie reviews for source data. The dataset comes already split into positive and negative labels, for classification, and into training and testing sets. We don't need any of these divisions, we just need all the text for training, so the data preparation steps here are:\n",
    "<ul>\n",
    "<li> Download the data.\n",
    "<li> Loop through all the files and generate a list of all the file names. \n",
    "<li> Crate a dataset from all the files. \n",
    "<li> Clean the data by removing the html tags and punctuation.\n",
    "<li> Tokenize the data by splitting the text into words and creating a vocabulary.\n",
    "<li> Create training ready data by creating sequences of X = \"up to the current word\" and Y = \"the next word\".\n",
    "<li> Set the dataset to be shuffled, batched, and prefetched.\n",
    "</ul>\n",
    "\n",
    "<b>Note:</b> there are a few odd [UNK] tokens, this is a placeholder for words that are not in the vocabulary. Were this a production model, we'd want to come up with some more sophisticated way of handling this, but for this example, we'll just leave it as is. When dealing with natural text, it is common to have things like this for unknown data, or other special tokens for the beginning or end of a sentence (e.g. [BOS] or [EOS]).  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\n",
      "                                 Dload  Upload   Total   Spent    Left  Speed\n",
      "100 80.2M  100 80.2M    0     0  3682k      0  0:00:22  0:00:22 --:--:-- 4804k0:34  0:00:10  0:00:24 3140k\n"
     ]
    }
   ],
   "source": [
    "!curl -O https://ai.stanford.edu/~amaas/data/sentiment/aclImdb_v1.tar.gz\n",
    "!tar -xf aclImdb_v1.tar.gz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "50000 files\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-04-19 11:40:18.439331: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "batch_size = 256\n",
    "EPOCHS = 20\n",
    "\n",
    "# The dataset contains each review in a separate text file\n",
    "# The text files are present in four different folders\n",
    "# Create a list all files\n",
    "filenames = []\n",
    "directories = [\n",
    "    \"aclImdb/train/pos\",\n",
    "    \"aclImdb/train/neg\",\n",
    "    \"aclImdb/test/pos\",\n",
    "    \"aclImdb/test/neg\",\n",
    "]\n",
    "for dir in directories:\n",
    "    for f in os.listdir(dir):\n",
    "        filenames.append(os.path.join(dir, f))\n",
    "\n",
    "print(f\"{len(filenames)} files\")\n",
    "\n",
    "# Create a dataset from text files\n",
    "random.shuffle(filenames)\n",
    "text_ds = tf.data.TextLineDataset(filenames)\n",
    "text_ds = text_ds.shuffle(buffer_size=256)\n",
    "text_ds = text_ds.batch(batch_size)\n",
    "\n",
    "\n",
    "def custom_standardization(input_string):\n",
    "    \"\"\" Remove html line-break tags and handle punctuation \"\"\"\n",
    "    lowercased = tf.strings.lower(input_string)\n",
    "    stripped_html = tf.strings.regex_replace(lowercased, \"<br />\", \" \")\n",
    "    return tf.strings.regex_replace(stripped_html, f\"([{string.punctuation}])\", r\" \\1\")\n",
    "\n",
    "\n",
    "# Create a vectorization layer and adapt it to the text\n",
    "vectorize_layer = TextVectorization(\n",
    "    standardize=custom_standardization,\n",
    "    max_tokens=vocab_size - 1,\n",
    "    output_mode=\"int\",\n",
    "    output_sequence_length=maxlen + 1,\n",
    ")\n",
    "vectorize_layer.adapt(text_ds)\n",
    "vocab = vectorize_layer.get_vocabulary()  # To get words back from token indices\n",
    "\n",
    "\n",
    "def prepare_lm_inputs_labels(text):\n",
    "    \"\"\"\n",
    "    Shift word sequences by 1 position so that the target for position (i) is\n",
    "    word at position (i+1). The model will use all words up till position (i)\n",
    "    to predict the next word.\n",
    "    \"\"\"\n",
    "    text = tf.expand_dims(text, -1)\n",
    "    tokenized_sentences = vectorize_layer(text)\n",
    "    x = tokenized_sentences[:, :-1]\n",
    "    y = tokenized_sentences[:, 1:]\n",
    "    return x, y\n",
    "\n",
    "\n",
    "text_ds = text_ds.map(prepare_lm_inputs_labels)\n",
    "text_ds = text_ds.prefetch(tf.data.AUTOTUNE)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can look at one example of the data below. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(256, 80) (256, 80)\n",
      "Tokens: [   12   218    13    22    37     2  3545 13500  2055    22  1384     3\n",
      "    13    22    74    84  1121    11     2     1  2028     8 16538    45\n",
      "    75    12   439    45    43    12    16   192   289   994     8    13\n",
      "    18     3    12    93   439  4251  1060     4 11159  7792   534     3\n",
      "     3     3    43    12    32    74    57  1163  2449    19  1084   956\n",
      "   329 13057   115     3   492     4    12   262    14 15583  3038   134\n",
      "    28  1643     1    19   109     7   145   108] \n",
      "\n",
      " [  218    13    22    37     2  3545 13500  2055    22  1384     3    13\n",
      "    22    74    84  1121    11     2     1  2028     8 16538    45    75\n",
      "    12   439    45    43    12    16   192   289   994     8    13    18\n",
      "     3    12    93   439  4251  1060     4 11159  7792   534     3     3\n",
      "     3    43    12    32    74    57  1163  2449    19  1084   956   329\n",
      " 13057   115     3   492     4    12   262    14 15583  3038   134    28\n",
      "  1643     1    19   109     7   145   108     3]\n",
      "Sentence: i saw this film at the 2005 edinburgh international film festival . this film had been compared in the [UNK] program to sideways - which i liked - so i was quite looking forward to this movie . i also liked garden state , napoleon dynamite etc . . . so i have had good recent experiences with slightly weird american indy films . unfortunately , i found that puffy chair does not compare [UNK] with any of these movies  \n",
      "\n",
      "Next word: .\n"
     ]
    }
   ],
   "source": [
    "tmp = text_ds.as_numpy_iterator()\n",
    "x_tmp, y_tmp = next(tmp)\n",
    "print(x_tmp.shape, y_tmp.shape)\n",
    "samp_x = x_tmp[0]\n",
    "samp_y = y_tmp[0]\n",
    "print(\"Tokens:\", samp_x, \"\\n\\n\", samp_y)\n",
    "word = \"\"\n",
    "for x_ in samp_x:\n",
    "    word += vocab[x_] + \" \"\n",
    "print(\"Sentence:\", word, \"\\n\\nNext word:\", vocab[samp_y[-1]])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Callback\n",
    "\n",
    "To get our text out, we can use a callback that will be called at the end of each epoch. We can still get things from \"predict\" after the fact, but this will give us some step by step evidence of our program's smarts. We will make two instances of this callback, each with different seeds. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TextGenerator(keras.callbacks.Callback):\n",
    "    \"\"\"A callback to generate text from a trained model.\n",
    "    1. Feed some starting prompt to the model\n",
    "    2. Predict probabilities for the next token\n",
    "    3. Sample the next token and add it to the next input\n",
    "\n",
    "    Arguments:\n",
    "        max_tokens: Integer, the number of tokens to be generated after prompt.\n",
    "        start_tokens: List of integers, the token indices for the starting prompt.\n",
    "        index_to_word: List of strings, obtained from the TextVectorization layer.\n",
    "        top_k: Integer, sample from the `top_k` token predictions.\n",
    "        print_every: Integer, print after this many epochs.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, max_tokens, start_tokens, index_to_word, top_k=10, print_every=1, log_dir=\"logs\"):\n",
    "        self.max_tokens = max_tokens\n",
    "        self.start_tokens = start_tokens\n",
    "        self.index_to_word = index_to_word\n",
    "        self.print_every = print_every\n",
    "        self.k = top_k\n",
    "        self.log_dir = log_dir\n",
    "\n",
    "    def sample_from(self, logits):\n",
    "        logits, indices = tf.math.top_k(logits, k=self.k, sorted=True)\n",
    "        indices = np.asarray(indices).astype(\"int32\")\n",
    "        preds = keras.activations.softmax(tf.expand_dims(logits, 0))[0]\n",
    "        preds = np.asarray(preds).astype(\"float32\")\n",
    "        return np.random.choice(indices, p=preds)\n",
    "\n",
    "    def detokenize(self, number):\n",
    "        return self.index_to_word[number]\n",
    "\n",
    "    def on_epoch_end(self, epoch, logs=None):\n",
    "        start_tokens = [_ for _ in self.start_tokens]\n",
    "        #if (epoch + 1) % self.print_every != 0:\n",
    "        #    return\n",
    "        num_tokens_generated = 0\n",
    "        tokens_generated = []\n",
    "        while num_tokens_generated <= self.max_tokens:\n",
    "            pad_len = maxlen - len(start_tokens)\n",
    "            sample_index = len(start_tokens) - 1\n",
    "            if pad_len < 0:\n",
    "                x = start_tokens[:maxlen]\n",
    "                sample_index = maxlen - 1\n",
    "            elif pad_len > 0:\n",
    "                x = start_tokens + [0] * pad_len\n",
    "            else:\n",
    "                x = start_tokens\n",
    "            x = np.array([x])\n",
    "            y, _ = self.model.predict(x)\n",
    "            sample_token = self.sample_from(y[0][sample_index])\n",
    "            tokens_generated.append(sample_token)\n",
    "            start_tokens.append(sample_token)\n",
    "            num_tokens_generated = len(tokens_generated)\n",
    "        txt = \" \".join(\n",
    "            [self.detokenize(_) for _ in self.start_tokens + tokens_generated]\n",
    "        )\n",
    "        print(f\"generated text:\\n{txt}\\n\")\n",
    "        file_writer = tf.summary.create_file_writer(self.log_dir)\n",
    "        with file_writer.as_default():\n",
    "            tf.summary.text(\"Text Data\", txt, step=epoch)\n",
    "        file_writer.flush()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train, Run, Predict\n",
    "\n",
    "Now that the model is created, we can fit it to the training data then test out the abilities. Our prediction is an incremental process, we start with a seed, then we predict the next word, then we add that word to the seed, and predict the next word, and so on. At each step, the model looks at the input to this point, calculates the attention, finds the most suitable (highest score) word from the vocabulary, generates it, and moves one more step forward. \n",
    "\n",
    "<b>Note:</b> Trying to train this on my laptop on CPU took forever, I didn't get to the point where the first epoch gave me a time estimate. On GPU it is much, much faster."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tokenize starting prompt\n",
    "word_to_index = {}\n",
    "for index, word in enumerate(vocab):\n",
    "    word_to_index[word] = index\n",
    "\n",
    "start_prompt1 = \"this movie is\"\n",
    "start_tokens1 = [word_to_index.get(_, 1) for _ in start_prompt1.split()]\n",
    "start_prompt2 = \"Skiing fast makes me\"\n",
    "start_tokens2 = [word_to_index.get(_, 1) for _ in start_prompt2.split()]\n",
    "num_tokens_generated = 40\n",
    "\n",
    "log_dir = \"logs/text\"\n",
    "text_gen_callback1 = TextGenerator(num_tokens_generated, start_tokens1, vocab, log_dir=log_dir)\n",
    "text_gen_callback2 = TextGenerator(num_tokens_generated, start_tokens2, vocab, log_dir=log_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Launch TensorBoard\n",
    "%load_ext tensorboard\n",
    "%tensorboard --logdir log_dir\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m/var/folders/px/vhm_920n7zx2wvqq_ht0q5tm0000gp/T/ipykernel_63994/2654523382.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcreate_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtext_ds\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mverbose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mEPOCHS\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcallbacks\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mtext_gen_callback1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtext_gen_callback2\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/opt/anaconda3/envs/ml3950/lib/python3.9/site-packages/keras/utils/traceback_utils.py\u001b[0m in \u001b[0;36merror_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     63\u001b[0m         \u001b[0mfiltered_tb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     64\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 65\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     66\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     67\u001b[0m             \u001b[0mfiltered_tb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_process_traceback_frames\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__traceback__\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/envs/ml3950/lib/python3.9/site-packages/keras/engine/training.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[1;32m   1648\u001b[0m                         ):\n\u001b[1;32m   1649\u001b[0m                             \u001b[0mcallbacks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mon_train_batch_begin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1650\u001b[0;31m                             \u001b[0mtmp_logs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0miterator\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1651\u001b[0m                             \u001b[0;32mif\u001b[0m \u001b[0mdata_handler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshould_sync\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1652\u001b[0m                                 \u001b[0mcontext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0masync_wait\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/envs/ml3950/lib/python3.9/site-packages/tensorflow/python/util/traceback_utils.py\u001b[0m in \u001b[0;36merror_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    148\u001b[0m     \u001b[0mfiltered_tb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    149\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 150\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    151\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    152\u001b[0m       \u001b[0mfiltered_tb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_process_traceback_frames\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__traceback__\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/envs/ml3950/lib/python3.9/site-packages/tensorflow/python/eager/polymorphic_function/polymorphic_function.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    878\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    879\u001b[0m       \u001b[0;32mwith\u001b[0m \u001b[0mOptionalXlaContext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jit_compile\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 880\u001b[0;31m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    881\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    882\u001b[0m       \u001b[0mnew_tracing_count\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexperimental_get_tracing_count\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/envs/ml3950/lib/python3.9/site-packages/tensorflow/python/eager/polymorphic_function/polymorphic_function.py\u001b[0m in \u001b[0;36m_call\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    910\u001b[0m       \u001b[0;31m# In this case we have created variables on the first call, so we run the\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    911\u001b[0m       \u001b[0;31m# defunned version which is guaranteed to never create variables.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 912\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_no_variable_creation_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# pylint: disable=not-callable\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    913\u001b[0m     \u001b[0;32melif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_variable_creation_fn\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    914\u001b[0m       \u001b[0;31m# Release the lock early so that multiple threads can perform the call\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/envs/ml3950/lib/python3.9/site-packages/tensorflow/python/eager/polymorphic_function/tracing_compiler.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    132\u001b[0m       (concrete_function,\n\u001b[1;32m    133\u001b[0m        filtered_flat_args) = self._maybe_define_function(args, kwargs)\n\u001b[0;32m--> 134\u001b[0;31m     return concrete_function._call_flat(\n\u001b[0m\u001b[1;32m    135\u001b[0m         filtered_flat_args, captured_inputs=concrete_function.captured_inputs)  # pylint: disable=protected-access\n\u001b[1;32m    136\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/envs/ml3950/lib/python3.9/site-packages/tensorflow/python/eager/polymorphic_function/monomorphic_function.py\u001b[0m in \u001b[0;36m_call_flat\u001b[0;34m(self, args, captured_inputs, cancellation_manager)\u001b[0m\n\u001b[1;32m   1743\u001b[0m         and executing_eagerly):\n\u001b[1;32m   1744\u001b[0m       \u001b[0;31m# No tape is watching; skip to running the function.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1745\u001b[0;31m       return self._build_call_outputs(self._inference_function.call(\n\u001b[0m\u001b[1;32m   1746\u001b[0m           ctx, args, cancellation_manager=cancellation_manager))\n\u001b[1;32m   1747\u001b[0m     forward_backward = self._select_forward_and_backward_functions(\n",
      "\u001b[0;32m~/opt/anaconda3/envs/ml3950/lib/python3.9/site-packages/tensorflow/python/eager/polymorphic_function/monomorphic_function.py\u001b[0m in \u001b[0;36mcall\u001b[0;34m(self, ctx, args, cancellation_manager)\u001b[0m\n\u001b[1;32m    376\u001b[0m       \u001b[0;32mwith\u001b[0m \u001b[0m_InterpolateFunctionError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    377\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mcancellation_manager\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 378\u001b[0;31m           outputs = execute.execute(\n\u001b[0m\u001b[1;32m    379\u001b[0m               \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msignature\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    380\u001b[0m               \u001b[0mnum_outputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_num_outputs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/envs/ml3950/lib/python3.9/site-packages/tensorflow/python/eager/execute.py\u001b[0m in \u001b[0;36mquick_execute\u001b[0;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[1;32m     50\u001b[0m   \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     51\u001b[0m     \u001b[0mctx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mensure_initialized\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 52\u001b[0;31m     tensors = pywrap_tfe.TFE_Py_Execute(ctx._handle, device_name, op_name,\n\u001b[0m\u001b[1;32m     53\u001b[0m                                         inputs, attrs, num_outputs)\n\u001b[1;32m     54\u001b[0m   \u001b[0;32mexcept\u001b[0m \u001b[0mcore\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_NotOkStatusException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "model = create_model()\n",
    "model.fit(text_ds, verbose=0, epochs=EPOCHS, callbacks=[text_gen_callback1, text_gen_callback2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def indToSentence(ind, dict, length=40):\n",
    "    word = \"\"\n",
    "    for n_ in ind:\n",
    "        word += dict[n_] + \" \"\n",
    "    return word\n",
    "\n",
    "# def sentenceToInd(sentence, dict, length=40):\n",
    "#     indicies = []\n",
    "#     words = sentence.split()\n",
    "#     for word in words:\n",
    "#         indicies.append(dict[word])\n",
    "#     return indicies\n",
    "\n",
    "def sentenceToInd(sentence, dict, length=40):\n",
    "    indicies = []\n",
    "    words = sentence.split()\n",
    "    for word in words:\n",
    "        if isinstance(word, str):\n",
    "            index = dict.get(word)\n",
    "            if index is not None:\n",
    "                indicies.append(index)\n",
    "    return indicies\n",
    "\n",
    "def sample_from(self, logits):\n",
    "    logits, indices = tf.math.top_k(logits, k=self.k, sorted=True)\n",
    "    indices = np.asarray(indices).astype(\"int32\")\n",
    "    preds = keras.activations.softmax(tf.expand_dims(logits, 0))[0]\n",
    "    preds = np.asarray(preds).astype(\"float32\")\n",
    "    return np.random.choice(indices, p=preds)\n",
    "\n",
    "def generateText(model, index_to_word, word_to_index, startPrompt, length=40):\n",
    "    start_tokens = sentenceToInd(startPrompt, word_to_index)\n",
    "    num_tokens_generated = 0\n",
    "    tokens_generated = []\n",
    "    while num_tokens_generated <= length:\n",
    "        pad_len = maxlen - len(start_tokens)\n",
    "        sample_index = len(start_tokens) - 1\n",
    "        if pad_len < 0:\n",
    "            x = start_tokens[:maxlen]\n",
    "            sample_index = maxlen - 1\n",
    "        elif pad_len > 0:\n",
    "            x = start_tokens + [0] * pad_len\n",
    "        else:\n",
    "            x = start_tokens\n",
    "        x = np.array([x])\n",
    "        y, _ = model.predict(x)\n",
    "        #sample_token = np.argmax(y[0][sample_index])\n",
    "        logits = y[0][sample_index]\n",
    "        sample_token = sample_from(logits)\n",
    "        tokens_generated.append(sample_token)\n",
    "        start_tokens.append(sample_token)\n",
    "        num_tokens_generated = len(tokens_generated)\n",
    "    txt = indToSentence(start_tokens + tokens_generated, index_to_word)\n",
    "    return txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "t1 = generateText(model, vocab, word_to_index, \"this movie is\")\n",
    "t2 = generateText(model, vocab, word_to_index, \"Skiing fast makes me\")\n",
    "t3 = generateText(model, vocab, word_to_index, \"We are going to make this country great\")\n",
    "t4 = generateText(model, vocab, word_to_index, \"Where my dogs at\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(t1)\n",
    "print(t2)\n",
    "print(t3)\n",
    "print(t4)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ml3950",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "4d722d3adfa415172c1f5238b519fb86b488acdae450fd691ab06c09f4ca9173"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
